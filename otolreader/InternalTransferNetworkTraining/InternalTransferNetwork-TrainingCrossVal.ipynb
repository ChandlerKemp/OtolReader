{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker as sage\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sage.Session()\n",
    "role = get_execution_role()\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class EstimatorBase in module sagemaker.estimator:\n",
      "\n",
      "class EstimatorBase(builtins.object)\n",
      " |  Handle end-to-end Amazon SageMaker training and deployment tasks.\n",
      " |  \n",
      " |  For introduction to model training and deployment, see\n",
      " |  http://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html\n",
      " |  \n",
      " |  Subclasses must define a way to determine what image to use for training,\n",
      " |  what hyperparameters to use, and how to create an appropriate predictor\n",
      " |  instance.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, role, instance_count=None, instance_type=None, volume_size=30, volume_kms_key=None, max_run=86400, input_mode='File', output_path=None, output_kms_key=None, base_job_name=None, sagemaker_session=None, tags=None, subnets=None, security_group_ids=None, model_uri=None, model_channel_name='model', metric_definitions=None, encrypt_inter_container_traffic=False, use_spot_instances=False, max_wait=None, checkpoint_s3_uri=None, checkpoint_local_path=None, rules=None, debugger_hook_config=None, tensorboard_output_config=None, enable_sagemaker_metrics=None, enable_network_isolation=False, profiler_config=None, disable_profiler=False, **kwargs)\n",
      " |      Initialize an ``EstimatorBase`` instance.\n",
      " |      \n",
      " |      Args:\n",
      " |          role (str): An AWS IAM role (either name or full ARN). The Amazon\n",
      " |              SageMaker training jobs and APIs that create Amazon SageMaker\n",
      " |              endpoints use this role to access training data and model\n",
      " |              artifacts. After the endpoint is created, the inference code\n",
      " |              might use the IAM role, if it needs to access an AWS resource.\n",
      " |          instance_count (int): Number of Amazon EC2 instances to use\n",
      " |              for training.\n",
      " |          instance_type (str): Type of EC2 instance to use for training,\n",
      " |              for example, 'ml.c4.xlarge'.\n",
      " |          volume_size (int): Size in GB of the EBS volume to use for\n",
      " |              storing input data during training (default: 30). Must be large\n",
      " |              enough to store training data if File Mode is used (which is the\n",
      " |              default).\n",
      " |          volume_kms_key (str): Optional. KMS key ID for encrypting EBS\n",
      " |              volume attached to the training instance (default: None).\n",
      " |          max_run (int): Timeout in seconds for training (default: 24 *\n",
      " |              60 * 60). After this amount of time Amazon SageMaker terminates\n",
      " |              the job regardless of its current status.\n",
      " |          input_mode (str): The input mode that the algorithm supports\n",
      " |              (default: 'File'). Valid modes: 'File' - Amazon SageMaker copies\n",
      " |              the training dataset from the S3 location to a local directory.\n",
      " |              'Pipe' - Amazon SageMaker streams data directly from S3 to the\n",
      " |              container via a Unix-named pipe. This argument can be overriden\n",
      " |              on a per-channel basis using\n",
      " |              ``sagemaker.inputs.TrainingInput.input_mode``.\n",
      " |          output_path (str): S3 location for saving the training result (model\n",
      " |              artifacts and output files). If not specified, results are\n",
      " |              stored to a default bucket. If the bucket with the specific name\n",
      " |              does not exist, the estimator creates the bucket during the\n",
      " |              :meth:`~sagemaker.estimator.EstimatorBase.fit` method execution.\n",
      " |              file:// urls are used for local mode. For example: 'file://model/'\n",
      " |              will save to the model folder in the current directory.\n",
      " |          output_kms_key (str): Optional. KMS key ID for encrypting the\n",
      " |              training output (default: None).\n",
      " |          base_job_name (str): Prefix for training job name when the\n",
      " |              :meth:`~sagemaker.estimator.EstimatorBase.fit` method launches.\n",
      " |              If not specified, the estimator generates a default job name\n",
      " |              based on the training image name and current timestamp.\n",
      " |          sagemaker_session (sagemaker.session.Session): Session object which\n",
      " |              manages interactions with Amazon SageMaker APIs and any other\n",
      " |              AWS services needed. If not specified, the estimator creates one\n",
      " |              using the default AWS configuration chain.\n",
      " |          tags (list[dict]): List of tags for labeling a training job. For\n",
      " |              more, see\n",
      " |              https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.\n",
      " |          subnets (list[str]): List of subnet ids. If not specified training\n",
      " |              job will be created without VPC config.\n",
      " |          security_group_ids (list[str]): List of security group ids. If not\n",
      " |              specified training job will be created without VPC config.\n",
      " |          model_uri (str): URI where a pre-trained model is stored, either\n",
      " |              locally or in S3 (default: None). If specified, the estimator\n",
      " |              will create a channel pointing to the model so the training job\n",
      " |              can download it. This model can be a 'model.tar.gz' from a\n",
      " |              previous training job, or other artifacts coming from a\n",
      " |              different source.\n",
      " |      \n",
      " |              In local mode, this should point to the path in which the model\n",
      " |              is located and not the file itself, as local Docker containers\n",
      " |              will try to mount the URI as a volume.\n",
      " |      \n",
      " |              More information:\n",
      " |              https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html#td-deserialization\n",
      " |          model_channel_name (str): Name of the channel where 'model_uri' will\n",
      " |              be downloaded (default: 'model').\n",
      " |          metric_definitions (list[dict]): A list of dictionaries that defines\n",
      " |              the metric(s) used to evaluate the training jobs. Each\n",
      " |              dictionary contains two keys: 'Name' for the name of the metric,\n",
      " |              and 'Regex' for the regular expression used to extract the\n",
      " |              metric from the logs. This should be defined only for jobs that\n",
      " |              don't use an Amazon algorithm.\n",
      " |          encrypt_inter_container_traffic (bool): Specifies whether traffic\n",
      " |              between training containers is encrypted for the training job\n",
      " |              (default: ``False``).\n",
      " |          use_spot_instances (bool): Specifies whether to use SageMaker\n",
      " |              Managed Spot instances for training. If enabled then the\n",
      " |              ``max_wait`` arg should also be set.\n",
      " |      \n",
      " |              More information:\n",
      " |              https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html\n",
      " |              (default: ``False``).\n",
      " |          max_wait (int): Timeout in seconds waiting for spot training\n",
      " |              job (default: None). After this amount of time Amazon\n",
      " |              SageMaker will stop waiting for managed spot training job to\n",
      " |              complete (default: ``None``).\n",
      " |          checkpoint_s3_uri (str): The S3 URI in which to persist checkpoints\n",
      " |              that the algorithm persists (if any) during training. (default:\n",
      " |              ``None``).\n",
      " |          checkpoint_local_path (str): The local path that the algorithm\n",
      " |              writes its checkpoints to. SageMaker will persist all files\n",
      " |              under this path to `checkpoint_s3_uri` continually during\n",
      " |              training. On job startup the reverse happens - data from the\n",
      " |              s3 location is downloaded to this path before the algorithm is\n",
      " |              started. If the path is unset then SageMaker assumes the\n",
      " |              checkpoints will be provided under `/opt/ml/checkpoints/`.\n",
      " |              (default: ``None``).\n",
      " |          rules (list[:class:`~sagemaker.debugger.RuleBase`]): A list of\n",
      " |              :class:`~sagemaker.debugger.RuleBase` objects used to define\n",
      " |              SageMaker Debugger rules for real-time analysis\n",
      " |              (default: ``None``). For more information,\n",
      " |              see `Continuous analyses through rules\n",
      " |              <https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_debugger.html\n",
      " |              #continuous-analyses-through-rules)>`_.\n",
      " |          debugger_hook_config (:class:`~sagemaker.debugger.DebuggerHookConfig` or bool):\n",
      " |              Configuration for how debugging information is emitted with\n",
      " |              SageMaker Debugger. If not specified, a default one is created using\n",
      " |              the estimator's ``output_path``, unless the region does not\n",
      " |              support SageMaker Debugger. To disable SageMaker Debugger,\n",
      " |              set this parameter to ``False``. For more information, see\n",
      " |              `Capture real-time debugging data during model training in Amazon SageMaker\n",
      " |              <https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_debugger.html#\n",
      " |              capture-real-time-debugging-data-during-model-training-in-amazon-sagemaker>`_.\n",
      " |          tensorboard_output_config (:class:`~sagemaker.debugger.TensorBoardOutputConfig`):\n",
      " |              Configuration for customizing debugging visualization using TensorBoard\n",
      " |              (default: ``None``). For more information,\n",
      " |              see `Capture real time tensorboard data\n",
      " |              <https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_debugger.html#\n",
      " |              capture-real-time-tensorboard-data-from-the-debugging-hook>`_.\n",
      " |          enable_sagemaker_metrics (bool): enable SageMaker Metrics Time\n",
      " |              Series. For more information, see `AlgorithmSpecification API\n",
      " |              <https://docs.aws.amazon.com/sagemaker/latest/dg/\n",
      " |              API_AlgorithmSpecification.html#SageMaker-Type-AlgorithmSpecification-\n",
      " |              EnableSageMakerMetricsTimeSeries>`_.\n",
      " |              (default: ``None``).\n",
      " |          enable_network_isolation (bool): Specifies whether container will\n",
      " |              run in network isolation mode (default: ``False``). Network\n",
      " |              isolation mode restricts the container access to outside networks\n",
      " |              (such as the Internet). The container does not make any inbound or\n",
      " |              outbound network calls. Also known as Internet-free mode.\n",
      " |          profiler_config (:class:`~sagemaker.debugger.ProfilerConfig`):\n",
      " |              Configuration for how SageMaker Debugger collects\n",
      " |              monitoring and profiling information from your training job.\n",
      " |              If not specified, a default configuration is created using\n",
      " |              the estimator's ``output_path``, unless the region does not\n",
      " |              support SageMaker Debugger. To disable SageMaker Debugger\n",
      " |              monitoring and profiling, set the\n",
      " |              ``disable_profiler`` parameter to ``True``.\n",
      " |          disable_profiler (bool): Specifies whether Debugger monitoring and profiling\n",
      " |              will be disabled (default: ``False``).\n",
      " |  \n",
      " |  compile_model(self, target_instance_family, input_shape, output_path, framework=None, framework_version=None, compile_max_run=900, tags=None, target_platform_os=None, target_platform_arch=None, target_platform_accelerator=None, compiler_options=None, **kwargs)\n",
      " |      Compile a Neo model using the input model.\n",
      " |      \n",
      " |      Args:\n",
      " |          target_instance_family (str): Identifies the device that you want to\n",
      " |              run your model after compilation, for example: ml_c5. For allowed\n",
      " |              strings see\n",
      " |              https://docs.aws.amazon.com/sagemaker/latest/dg/API_OutputConfig.html.\n",
      " |          input_shape (dict): Specifies the name and shape of the expected\n",
      " |              inputs for your trained model in json dictionary form, for\n",
      " |              example: {'data':[1,3,1024,1024]}, or {'var1': [1,1,28,28],\n",
      " |              'var2':[1,1,28,28]}\n",
      " |          output_path (str): Specifies where to store the compiled model\n",
      " |          framework (str): The framework that is used to train the original\n",
      " |              model. Allowed values: 'mxnet', 'tensorflow', 'keras', 'pytorch',\n",
      " |              'onnx', 'xgboost'\n",
      " |          framework_version (str): The version of the framework\n",
      " |          compile_max_run (int): Timeout in seconds for compilation (default:\n",
      " |              3 * 60). After this amount of time Amazon SageMaker Neo\n",
      " |              terminates the compilation job regardless of its current status.\n",
      " |          tags (list[dict]): List of tags for labeling a compilation job. For\n",
      " |              more, see\n",
      " |              https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.\n",
      " |          target_platform_os (str): Target Platform OS, for example: 'LINUX'.\n",
      " |              For allowed strings see\n",
      " |              https://docs.aws.amazon.com/sagemaker/latest/dg/API_OutputConfig.html.\n",
      " |              It can be used instead of target_instance_family.\n",
      " |          target_platform_arch (str): Target Platform Architecture, for example: 'X86_64'.\n",
      " |              For allowed strings see\n",
      " |              https://docs.aws.amazon.com/sagemaker/latest/dg/API_OutputConfig.html.\n",
      " |              It can be used instead of target_instance_family.\n",
      " |          target_platform_accelerator (str, optional): Target Platform Accelerator,\n",
      " |              for example: 'NVIDIA'. For allowed strings see\n",
      " |              https://docs.aws.amazon.com/sagemaker/latest/dg/API_OutputConfig.html.\n",
      " |              It can be used instead of target_instance_family.\n",
      " |          compiler_options (dict, optional): Additional parameters for compiler.\n",
      " |              Compiler Options are TargetPlatform / target_instance_family specific. See\n",
      " |              https://docs.aws.amazon.com/sagemaker/latest/dg/API_OutputConfig.html for details.\n",
      " |          **kwargs: Passed to invocation of ``create_model()``.\n",
      " |              Implementations may customize ``create_model()`` to accept\n",
      " |              ``**kwargs`` to customize model creation during deploy. For\n",
      " |              more, see the implementation docs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          sagemaker.model.Model: A SageMaker ``Model`` object. See\n",
      " |          :func:`~sagemaker.model.Model` for full details.\n",
      " |  \n",
      " |  create_model(self, **kwargs)\n",
      " |      Create a SageMaker ``Model`` object that can be deployed to an ``Endpoint``.\n",
      " |      \n",
      " |      Args:\n",
      " |          **kwargs: Keyword arguments used by the implemented method for\n",
      " |              creating the ``Model``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          sagemaker.model.Model: A SageMaker ``Model`` object. See\n",
      " |          :func:`~sagemaker.model.Model` for full details.\n",
      " |  \n",
      " |  delete_endpoint = func(*args, **kwargs)\n",
      " |  \n",
      " |  deploy(self, initial_instance_count, instance_type, serializer=None, deserializer=None, accelerator_type=None, endpoint_name=None, use_compiled_model=False, wait=True, model_name=None, kms_key=None, data_capture_config=None, tags=None, **kwargs)\n",
      " |      Deploy the trained model to an Amazon SageMaker endpoint.\n",
      " |      \n",
      " |       And then return ``sagemaker.Predictor`` object.\n",
      " |      \n",
      " |      More information:\n",
      " |      http://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html\n",
      " |      \n",
      " |      Args:\n",
      " |          initial_instance_count (int): Minimum number of EC2 instances to\n",
      " |              deploy to an endpoint for prediction.\n",
      " |          instance_type (str): Type of EC2 instance to deploy to an endpoint\n",
      " |              for prediction, for example, 'ml.c4.xlarge'.\n",
      " |          serializer (:class:`~sagemaker.serializers.BaseSerializer`): A\n",
      " |              serializer object, used to encode data for an inference endpoint\n",
      " |              (default: None). If ``serializer`` is not None, then\n",
      " |              ``serializer`` will override the default serializer. The\n",
      " |              default serializer is set by the ``predictor_cls``.\n",
      " |          deserializer (:class:`~sagemaker.deserializers.BaseDeserializer`): A\n",
      " |              deserializer object, used to decode data from an inference\n",
      " |              endpoint (default: None). If ``deserializer`` is not None, then\n",
      " |              ``deserializer`` will override the default deserializer. The\n",
      " |              default deserializer is set by the ``predictor_cls``.\n",
      " |          accelerator_type (str): Type of Elastic Inference accelerator to\n",
      " |              attach to an endpoint for model loading and inference, for\n",
      " |              example, 'ml.eia1.medium'. If not specified, no Elastic\n",
      " |              Inference accelerator will be attached to the endpoint. For more\n",
      " |              information:\n",
      " |              https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html\n",
      " |          endpoint_name (str): Name to use for creating an Amazon SageMaker\n",
      " |              endpoint. If not specified, the name of the training job is\n",
      " |              used.\n",
      " |          use_compiled_model (bool): Flag to select whether to use compiled\n",
      " |              (optimized) model. Default: False.\n",
      " |          wait (bool): Whether the call should wait until the deployment of\n",
      " |              model completes (default: True).\n",
      " |          model_name (str): Name to use for creating an Amazon SageMaker\n",
      " |              model. If not specified, the estimator generates a default job name\n",
      " |              based on the training image name and current timestamp.\n",
      " |          kms_key (str): The ARN of the KMS key that is used to encrypt the\n",
      " |              data on the storage volume attached to the instance hosting the\n",
      " |              endpoint.\n",
      " |          data_capture_config (sagemaker.model_monitor.DataCaptureConfig): Specifies\n",
      " |              configuration related to Endpoint data capture for use with\n",
      " |              Amazon SageMaker Model Monitoring. Default: None.\n",
      " |          tags(List[dict[str, str]]): Optional. The list of tags to attach to this specific\n",
      " |              endpoint. Example:\n",
      " |              >>> tags = [{'Key': 'tagname', 'Value': 'tagvalue'}]\n",
      " |              For more information about tags, see\n",
      " |              https://boto3.amazonaws.com/v1/documentation                /api/latest/reference/services/sagemaker.html#SageMaker.Client.add_tags\n",
      " |          **kwargs: Passed to invocation of ``create_model()``.\n",
      " |              Implementations may customize ``create_model()`` to accept\n",
      " |              ``**kwargs`` to customize model creation during deploy.\n",
      " |              For more, see the implementation docs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          sagemaker.predictor.Predictor: A predictor that provides a ``predict()`` method,\n",
      " |              which can be used to send requests to the Amazon SageMaker\n",
      " |              endpoint and obtain inferences.\n",
      " |  \n",
      " |  disable_profiling(self)\n",
      " |      Update the current training job in progress to disable profiling.\n",
      " |      \n",
      " |      Debugger stops collecting the system and framework metrics\n",
      " |      and turns off the Debugger built-in monitoring and profiling rules.\n",
      " |  \n",
      " |  enable_default_profiling(self)\n",
      " |      Update training job to enable Debugger monitoring.\n",
      " |      \n",
      " |      This method enables Debugger monitoring with\n",
      " |      the default ``profiler_config`` parameter to collect system\n",
      " |      metrics and the default built-in ``profiler_report`` rule.\n",
      " |      Framework metrics won't be saved.\n",
      " |      To update training job to emit framework metrics, you can use\n",
      " |      :class:`~sagemaker.estimator.Estimator.update_profiler`\n",
      " |      method and specify the framework metrics you want to enable.\n",
      " |      \n",
      " |      This method is callable when the training job is in progress while\n",
      " |      Debugger monitoring is disabled.\n",
      " |  \n",
      " |  enable_network_isolation(self)\n",
      " |      Return True if this Estimator will need network isolation to run.\n",
      " |      \n",
      " |      Returns:\n",
      " |          bool: Whether this Estimator needs network isolation or not.\n",
      " |  \n",
      " |  fit(self, inputs=None, wait=True, logs='All', job_name=None, experiment_config=None)\n",
      " |      Train a model using the input training dataset.\n",
      " |      \n",
      " |      The API calls the Amazon SageMaker CreateTrainingJob API to start\n",
      " |      model training. The API uses configuration you provided to create the\n",
      " |      estimator and the specified input training data to send the\n",
      " |      CreatingTrainingJob request to Amazon SageMaker.\n",
      " |      \n",
      " |      This is a synchronous operation. After the model training\n",
      " |      successfully completes, you can call the ``deploy()`` method to host the\n",
      " |      model using the Amazon SageMaker hosting services.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs (str or dict or sagemaker.inputs.TrainingInput): Information\n",
      " |              about the training data. This can be one of three types:\n",
      " |      \n",
      " |              * (str) the S3 location where training data is saved, or a file:// path in\n",
      " |                  local mode.\n",
      " |              * (dict[str, str] or dict[str, sagemaker.inputs.TrainingInput]) If using multiple\n",
      " |                  channels for training data, you can specify a dict mapping channel names to\n",
      " |                  strings or :func:`~sagemaker.inputs.TrainingInput` objects.\n",
      " |              * (sagemaker.inputs.TrainingInput) - channel configuration for S3 data sources\n",
      " |                  that can provide additional information as well as the path to the training\n",
      " |                  dataset.\n",
      " |                  See :func:`sagemaker.inputs.TrainingInput` for full details.\n",
      " |              * (sagemaker.session.FileSystemInput) - channel configuration for\n",
      " |                  a file system data source that can provide additional information as well as\n",
      " |                  the path to the training dataset.\n",
      " |      \n",
      " |          wait (bool): Whether the call should wait until the job completes (default: True).\n",
      " |          logs ([str]): A list of strings specifying which logs to print. Acceptable\n",
      " |              strings are \"All\", \"None\", \"Training\", or \"Rules\". To maintain backwards\n",
      " |              compatibility, boolean values are also accepted and converted to strings.\n",
      " |              Only meaningful when wait is True.\n",
      " |          job_name (str): Training job name. If not specified, the estimator generates\n",
      " |              a default job name based on the training image name and current timestamp.\n",
      " |          experiment_config (dict[str, str]): Experiment management configuration.\n",
      " |              Dictionary contains three optional keys,\n",
      " |              'ExperimentName', 'TrialName', and 'TrialComponentDisplayName'.\n",
      " |  \n",
      " |  get_vpc_config(self, vpc_config_override='VPC_CONFIG_DEFAULT')\n",
      " |      Returns VpcConfig dict either from this Estimator's subnets and security groups.\n",
      " |      \n",
      " |      Or else validate and return an optional override value.\n",
      " |      \n",
      " |      Args:\n",
      " |          vpc_config_override:\n",
      " |  \n",
      " |  hyperparameters(self)\n",
      " |      Return the hyperparameters as a dictionary to use for training.\n",
      " |      \n",
      " |      The :meth:`~sagemaker.estimator.EstimatorBase.fit` method, which\n",
      " |      trains the model, calls this method to find the hyperparameters.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict[str, str]: The hyperparameters.\n",
      " |  \n",
      " |  latest_job_debugger_artifacts_path(self)\n",
      " |      Gets the path to the DebuggerHookConfig output artifacts.\n",
      " |      \n",
      " |      Returns:\n",
      " |          str: An S3 path to the output artifacts.\n",
      " |  \n",
      " |  latest_job_profiler_artifacts_path(self)\n",
      " |      Gets the path to the profiling output artifacts.\n",
      " |      \n",
      " |      Returns:\n",
      " |          str: An S3 path to the output artifacts.\n",
      " |  \n",
      " |  latest_job_tensorboard_artifacts_path(self)\n",
      " |      Gets the path to the TensorBoardOutputConfig output artifacts.\n",
      " |      \n",
      " |      Returns:\n",
      " |          str: An S3 path to the output artifacts.\n",
      " |  \n",
      " |  logs(self)\n",
      " |      Display the logs for Estimator's training job.\n",
      " |      \n",
      " |      If the output is a tty or a Jupyter cell, it will be color-coded based\n",
      " |      on which instance the log entry is from.\n",
      " |  \n",
      " |  prepare_workflow_for_training(self, job_name=None)\n",
      " |      Calls _prepare_for_training. Used when setting up a workflow.\n",
      " |      \n",
      " |      Args:\n",
      " |          job_name (str): Name of the training job to be created. If not\n",
      " |              specified, one is generated, using the base name given to the\n",
      " |              constructor if applicable.\n",
      " |  \n",
      " |  register(self, content_types, response_types, inference_instances, transform_instances, image_uri=None, model_package_name=None, model_package_group_name=None, model_metrics=None, metadata_properties=None, marketplace_cert=False, approval_status=None, description=None, compile_model_family=None, model_name=None, **kwargs)\n",
      " |      Creates a model package for creating SageMaker models or listing on Marketplace.\n",
      " |      \n",
      " |      Args:\n",
      " |          content_types (list): The supported MIME types for the input data.\n",
      " |          response_types (list): The supported MIME types for the output data.\n",
      " |          inference_instances (list): A list of the instance types that are used to\n",
      " |              generate inferences in real-time.\n",
      " |          transform_instances (list): A list of the instance types on which a transformation\n",
      " |              job can be run or on which an endpoint can be deployed.\n",
      " |          image_uri (str): The container image uri for Model Package, if not specified,\n",
      " |              Estimator's training container image will be used (default: None).\n",
      " |          model_package_name (str): Model Package name, exclusive to `model_package_group_name`,\n",
      " |              using `model_package_name` makes the Model Package un-versioned (default: None).\n",
      " |          model_package_group_name (str): Model Package Group name, exclusive to\n",
      " |              `model_package_name`, using `model_package_group_name` makes the Model Package\n",
      " |              versioned (default: None).\n",
      " |          model_metrics (ModelMetrics): ModelMetrics object (default: None).\n",
      " |          metadata_properties (MetadataProperties): MetadataProperties (default: None).\n",
      " |          marketplace_cert (bool): A boolean value indicating if the Model Package is certified\n",
      " |              for AWS Marketplace (default: False).\n",
      " |          approval_status (str): Model Approval Status, values can be \"Approved\", \"Rejected\",\n",
      " |              or \"PendingManualApproval\" (default: \"PendingManualApproval\").\n",
      " |          description (str): Model Package description (default: None).\n",
      " |          compile_model_family (str): Instance family for compiled model, if specified, a compiled\n",
      " |              model will be used (default: None).\n",
      " |          model_name (str): User defined model name (default: None).\n",
      " |          **kwargs: Passed to invocation of ``create_model()``. Implementations may customize\n",
      " |              ``create_model()`` to accept ``**kwargs`` to customize model creation during\n",
      " |              deploy. For more, see the implementation docs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          str: A string of SageMaker Model Package ARN.\n",
      " |  \n",
      " |  training_image_uri(self)\n",
      " |      Return the Docker image to use for training.\n",
      " |      \n",
      " |      The :meth:`~sagemaker.estimator.EstimatorBase.fit` method, which does\n",
      " |      the model training, calls this method to find the image to use for model\n",
      " |      training.\n",
      " |      \n",
      " |      Returns:\n",
      " |          str: The URI of the Docker image.\n",
      " |  \n",
      " |  transformer(self, instance_count, instance_type, strategy=None, assemble_with=None, output_path=None, output_kms_key=None, accept=None, env=None, max_concurrent_transforms=None, max_payload=None, tags=None, role=None, volume_kms_key=None, vpc_config_override='VPC_CONFIG_DEFAULT', enable_network_isolation=None, model_name=None)\n",
      " |      Return a ``Transformer`` that uses a SageMaker Model based on the training job.\n",
      " |      \n",
      " |      It reuses the SageMaker Session and base job name used by\n",
      " |      the Estimator.\n",
      " |      \n",
      " |      Args:\n",
      " |          instance_count (int): Number of EC2 instances to use.\n",
      " |          instance_type (str): Type of EC2 instance to use, for example,\n",
      " |              'ml.c4.xlarge'.\n",
      " |          strategy (str): The strategy used to decide how to batch records in\n",
      " |              a single request (default: None). Valid values: 'MultiRecord'\n",
      " |              and 'SingleRecord'.\n",
      " |          assemble_with (str): How the output is assembled (default: None).\n",
      " |              Valid values: 'Line' or 'None'.\n",
      " |          output_path (str): S3 location for saving the transform result. If\n",
      " |              not specified, results are stored to a default bucket.\n",
      " |          output_kms_key (str): Optional. KMS key ID for encrypting the\n",
      " |              transform output (default: None).\n",
      " |          accept (str): The accept header passed by the client to\n",
      " |              the inference endpoint. If it is supported by the endpoint,\n",
      " |              it will be the format of the batch transform output.\n",
      " |          env (dict): Environment variables to be set for use during the\n",
      " |              transform job (default: None).\n",
      " |          max_concurrent_transforms (int): The maximum number of HTTP requests\n",
      " |              to be made to each individual transform container at one time.\n",
      " |          max_payload (int): Maximum size of the payload in a single HTTP\n",
      " |              request to the container in MB.\n",
      " |          tags (list[dict]): List of tags for labeling a transform job. If\n",
      " |              none specified, then the tags used for the training job are used\n",
      " |              for the transform job.\n",
      " |          role (str): The ``ExecutionRoleArn`` IAM Role ARN for the ``Model``,\n",
      " |              which is also used during transform jobs. If not specified, the\n",
      " |              role from the Estimator will be used.\n",
      " |          volume_kms_key (str): Optional. KMS key ID for encrypting the volume\n",
      " |              attached to the ML compute instance (default: None).\n",
      " |          vpc_config_override (dict[str, list[str]]): Optional override for the\n",
      " |              VpcConfig set on the model.\n",
      " |              Default: use subnets and security groups from this Estimator.\n",
      " |      \n",
      " |              * 'Subnets' (list[str]): List of subnet ids.\n",
      " |              * 'SecurityGroupIds' (list[str]): List of security group ids.\n",
      " |      \n",
      " |          enable_network_isolation (bool): Specifies whether container will\n",
      " |              run in network isolation mode. Network isolation mode restricts\n",
      " |              the container access to outside networks (such as the internet).\n",
      " |              The container does not make any inbound or outbound network\n",
      " |              calls. If True, a channel named \"code\" will be created for any\n",
      " |              user entry script for inference. Also known as Internet-free mode.\n",
      " |              If not specified, this setting is taken from the estimator's\n",
      " |              current configuration.\n",
      " |          model_name (str): Name to use for creating an Amazon SageMaker\n",
      " |              model. If not specified, the estimator generates a default job name\n",
      " |              based on the training image name and current timestamp.\n",
      " |  \n",
      " |  update_profiler(self, rules=None, system_monitor_interval_millis=None, s3_output_path=None, framework_profile_params=None, disable_framework_metrics=False)\n",
      " |      Update training jobs to enable profiling.\n",
      " |      \n",
      " |      This method updates the ``profiler_config`` parameter\n",
      " |      and initiates Debugger built-in rules for profiling.\n",
      " |      \n",
      " |      Args:\n",
      " |          rules (list[:class:`~sagemaker.debugger.ProfilerRule`]): A list of\n",
      " |              :class:`~sagemaker.debugger.ProfilerRule` objects to define\n",
      " |              rules for continuous analysis with SageMaker Debugger. Currently, you can\n",
      " |              only add new profiler rules during the training job. (default: ``None``)\n",
      " |          s3_output_path (str): The location in S3 to store the output. If profiler is enabled\n",
      " |              once, s3_output_path cannot be changed. (default: ``None``)\n",
      " |          system_monitor_interval_millis (int): How often profiling system metrics are\n",
      " |              collected; Unit: Milliseconds (default: ``None``)\n",
      " |          framework_profile_params (:class:`~sagemaker.debugger.FrameworkProfile`):\n",
      " |              A parameter object for framework metrics profiling. Configure it using\n",
      " |              the :class:`~sagemaker.debugger.FrameworkProfile` class.\n",
      " |              To use the default framework profile parameters, pass ``FrameworkProfile()``.\n",
      " |              For more information about the default values,\n",
      " |              see :class:`~sagemaker.debugger.FrameworkProfile`. (default: ``None``)\n",
      " |          disable_framework_metrics (bool): Specify whether to disable all the framework metrics.\n",
      " |              This won't update system metrics and the Debugger built-in rules for monitoring.\n",
      " |              To stop both monitoring and profiling,\n",
      " |              use the :class:`~sagemaker.estimator.Estimator.desable_profiling`\n",
      " |              method. (default: ``False``)\n",
      " |      \n",
      " |      .. attention::\n",
      " |      \n",
      " |          Updating the profiling configuration for TensorFlow dataloader profiling\n",
      " |          is currently not available. If you started a TensorFlow training job only with\n",
      " |          monitoring and want to enable profiling while the training job is running,\n",
      " |          the dataloader profiling cannot be updated.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  attach(training_job_name, sagemaker_session=None, model_channel_name='model') from abc.ABCMeta\n",
      " |      Attach to an existing training job.\n",
      " |      \n",
      " |      Create an Estimator bound to an existing training job, each subclass\n",
      " |      is responsible to implement\n",
      " |      ``_prepare_init_params_from_job_description()`` as this method delegates\n",
      " |      the actual conversion of a training job description to the arguments\n",
      " |      that the class constructor expects. After attaching, if the training job\n",
      " |      has a Complete status, it can be ``deploy()`` ed to create a SageMaker\n",
      " |      Endpoint and return a ``Predictor``.\n",
      " |      \n",
      " |      If the training job is in progress, attach will block until the training job\n",
      " |      completes, but logs of the training job will not display. To see the logs\n",
      " |      content, please call ``logs()``\n",
      " |      \n",
      " |      Examples:\n",
      " |          >>> my_estimator.fit(wait=False)\n",
      " |          >>> training_job_name = my_estimator.latest_training_job.name\n",
      " |          Later on:\n",
      " |          >>> attached_estimator = Estimator.attach(training_job_name)\n",
      " |          >>> attached_estimator.logs()\n",
      " |          >>> attached_estimator.deploy()\n",
      " |      \n",
      " |      Args:\n",
      " |          training_job_name (str): The name of the training job to attach to.\n",
      " |          sagemaker_session (sagemaker.session.Session): Session object which\n",
      " |              manages interactions with Amazon SageMaker APIs and any other\n",
      " |              AWS services needed. If not specified, the estimator creates one\n",
      " |              using the default AWS configuration chain.\n",
      " |          model_channel_name (str): Name of the channel where pre-trained\n",
      " |              model data will be downloaded (default: 'model'). If no channel\n",
      " |              with the same name exists in the training job, this option will\n",
      " |              be ignored.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Instance of the calling ``Estimator`` Class with the attached\n",
      " |          training job.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  model_data\n",
      " |      str: The model location in S3. Only set if Estimator has been ``fit()``.\n",
      " |  \n",
      " |  training_job_analytics\n",
      " |      Return a ``TrainingJobAnalytics`` object for the current training job.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset({'create_model', 'hyperparameters', 't...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sage.estimator.EstimatorBase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Framework in module sagemaker.estimator:\n",
      "\n",
      "class Framework(EstimatorBase)\n",
      " |  Base class that cannot be instantiated directly.\n",
      " |  \n",
      " |  Subclasses define functionality pertaining to specific ML frameworks,\n",
      " |  such as training/deployment images and predictor instances.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Framework\n",
      " |      EstimatorBase\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, entry_point, source_dir=None, hyperparameters=None, container_log_level=20, code_location=None, image_uri=None, dependencies=None, enable_network_isolation=False, git_config=None, checkpoint_s3_uri=None, checkpoint_local_path=None, enable_sagemaker_metrics=None, **kwargs)\n",
      " |      Base class initializer.\n",
      " |      \n",
      " |      Subclasses which override ``__init__`` should invoke ``super()``.\n",
      " |      \n",
      " |      Args:\n",
      " |          entry_point (str): Path (absolute or relative) to the local Python\n",
      " |              source file which should be executed as the entry point to\n",
      " |              training. If ``source_dir`` is specified, then ``entry_point``\n",
      " |              must point to a file located at the root of ``source_dir``.\n",
      " |              If 'git_config' is provided, 'entry_point' should be\n",
      " |              a relative location to the Python source file in the Git repo.\n",
      " |      \n",
      " |              Example:\n",
      " |                  With the following GitHub repo directory structure:\n",
      " |      \n",
      " |                  >>> |----- README.md\n",
      " |                  >>> |----- src\n",
      " |                  >>>         |----- train.py\n",
      " |                  >>>         |----- test.py\n",
      " |      \n",
      " |                  You can assign entry_point='src/train.py'.\n",
      " |          source_dir (str): Path (absolute, relative or an S3 URI) to a directory\n",
      " |              with any other training source code dependencies aside from the entry\n",
      " |              point file (default: None). If ``source_dir`` is an S3 URI, it must\n",
      " |              point to a tar.gz file. Structure within this directory are preserved\n",
      " |              when training on Amazon SageMaker. If 'git_config' is provided,\n",
      " |              'source_dir' should be a relative location to a directory in the Git\n",
      " |              repo.\n",
      " |      \n",
      " |              .. admonition:: Example\n",
      " |      \n",
      " |                  With the following GitHub repo directory structure:\n",
      " |      \n",
      " |                  >>> |----- README.md\n",
      " |                  >>> |----- src\n",
      " |                  >>>         |----- train.py\n",
      " |                  >>>         |----- test.py\n",
      " |      \n",
      " |                  and you need 'train.py' as entry point and 'test.py' as\n",
      " |                  training source code as well, you can assign\n",
      " |                  entry_point='train.py', source_dir='src'.\n",
      " |          hyperparameters (dict): Hyperparameters that will be used for\n",
      " |              training (default: None). The hyperparameters are made\n",
      " |              accessible as a dict[str, str] to the training code on\n",
      " |              SageMaker. For convenience, this accepts other types for keys\n",
      " |              and values, but ``str()`` will be called to convert them before\n",
      " |              training.\n",
      " |          container_log_level (int): Log level to use within the container\n",
      " |              (default: logging.INFO). Valid values are defined in the Python\n",
      " |              logging module.\n",
      " |          code_location (str): The S3 prefix URI where custom code will be\n",
      " |              uploaded (default: None) - don't include a trailing slash since\n",
      " |              a string prepended with a \"/\" is appended to ``code_location``. The code\n",
      " |              file uploaded to S3 is 'code_location/job-name/source/sourcedir.tar.gz'.\n",
      " |              If not specified, the default ``code location`` is s3://output_bucket/job-name/.\n",
      " |          image_uri (str): An alternate image name to use instead of the\n",
      " |              official Sagemaker image for the framework. This is useful to\n",
      " |              run one of the Sagemaker supported frameworks with an image\n",
      " |              containing custom dependencies.\n",
      " |          dependencies (list[str]): A list of paths to directories (absolute\n",
      " |              or relative) with any additional libraries that will be exported\n",
      " |              to the container (default: []). The library folders will be\n",
      " |              copied to SageMaker in the same folder where the entrypoint is\n",
      " |              copied. If 'git_config' is provided, 'dependencies' should be a\n",
      " |              list of relative locations to directories with any additional\n",
      " |              libraries needed in the Git repo.\n",
      " |      \n",
      " |              .. admonition:: Example\n",
      " |      \n",
      " |                  The following call\n",
      " |      \n",
      " |                  >>> Estimator(entry_point='train.py',\n",
      " |                  ...           dependencies=['my/libs/common', 'virtual-env'])\n",
      " |      \n",
      " |                  results in the following inside the container:\n",
      " |      \n",
      " |                  >>> $ ls\n",
      " |      \n",
      " |                  >>> opt/ml/code\n",
      " |                  >>>     |------ train.py\n",
      " |                  >>>     |------ common\n",
      " |                  >>>     |------ virtual-env\n",
      " |      \n",
      " |              This is not supported with \"local code\" in Local Mode.\n",
      " |          enable_network_isolation (bool): Specifies whether container will\n",
      " |              run in network isolation mode. Network isolation mode restricts\n",
      " |              the container access to outside networks (such as the internet).\n",
      " |              The container does not make any inbound or outbound network\n",
      " |              calls. If True, a channel named \"code\" will be created for any\n",
      " |              user entry script for training. The user entry script, files in\n",
      " |              source_dir (if specified), and dependencies will be uploaded in\n",
      " |              a tar to S3. Also known as internet-free mode (default: `False`).\n",
      " |          git_config (dict[str, str]): Git configurations used for cloning\n",
      " |              files, including ``repo``, ``branch``, ``commit``,\n",
      " |              ``2FA_enabled``, ``username``, ``password`` and ``token``. The\n",
      " |              ``repo`` field is required. All other fields are optional.\n",
      " |              ``repo`` specifies the Git repository where your training script\n",
      " |              is stored. If you don't provide ``branch``, the default value\n",
      " |              'master' is used. If you don't provide ``commit``, the latest\n",
      " |              commit in the specified branch is used. .. admonition:: Example\n",
      " |      \n",
      " |                  The following config:\n",
      " |      \n",
      " |                  >>> git_config = {'repo': 'https://github.com/aws/sagemaker-python-sdk.git',\n",
      " |                  >>>               'branch': 'test-branch-git-config',\n",
      " |                  >>>               'commit': '329bfcf884482002c05ff7f44f62599ebc9f445a'}\n",
      " |      \n",
      " |                  results in cloning the repo specified in 'repo', then\n",
      " |                  checkout the 'master' branch, and checkout the specified\n",
      " |                  commit.\n",
      " |      \n",
      " |              ``2FA_enabled``, ``username``, ``password`` and ``token`` are\n",
      " |              used for authentication. For GitHub (or other Git) accounts, set\n",
      " |              ``2FA_enabled`` to 'True' if two-factor authentication is\n",
      " |              enabled for the account, otherwise set it to 'False'. If you do\n",
      " |              not provide a value for ``2FA_enabled``, a default value of\n",
      " |              'False' is used. CodeCommit does not support two-factor\n",
      " |              authentication, so do not provide \"2FA_enabled\" with CodeCommit\n",
      " |              repositories.\n",
      " |      \n",
      " |              For GitHub and other Git repos, when SSH URLs are provided, it\n",
      " |              doesn't matter whether 2FA is enabled or disabled; you should\n",
      " |              either have no passphrase for the SSH key pairs, or have the\n",
      " |              ssh-agent configured so that you will not be prompted for SSH\n",
      " |              passphrase when you do 'git clone' command with SSH URLs. When\n",
      " |              HTTPS URLs are provided: if 2FA is disabled, then either token\n",
      " |              or username+password will be used for authentication if provided\n",
      " |              (token prioritized); if 2FA is enabled, only token will be used\n",
      " |              for authentication if provided. If required authentication info\n",
      " |              is not provided, python SDK will try to use local credentials\n",
      " |              storage to authenticate. If that fails either, an error message\n",
      " |              will be thrown.\n",
      " |      \n",
      " |              For CodeCommit repos, 2FA is not supported, so '2FA_enabled'\n",
      " |              should not be provided. There is no token in CodeCommit, so\n",
      " |              'token' should not be provided too. When 'repo' is an SSH URL,\n",
      " |              the requirements are the same as GitHub-like repos. When 'repo'\n",
      " |              is an HTTPS URL, username+password will be used for\n",
      " |              authentication if they are provided; otherwise, python SDK will\n",
      " |              try to use either CodeCommit credential helper or local\n",
      " |              credential storage for authentication.\n",
      " |          checkpoint_s3_uri (str): The S3 URI in which to persist checkpoints\n",
      " |              that the algorithm persists (if any) during training. (default:\n",
      " |              ``None``).\n",
      " |          checkpoint_local_path (str): The local path that the algorithm\n",
      " |              writes its checkpoints to. SageMaker will persist all files\n",
      " |              under this path to `checkpoint_s3_uri` continually during\n",
      " |              training. On job startup the reverse happens - data from the\n",
      " |              s3 location is downloaded to this path before the algorithm is\n",
      " |              started. If the path is unset then SageMaker assumes the\n",
      " |              checkpoints will be provided under `/opt/ml/checkpoints/`.\n",
      " |              (default: ``None``).\n",
      " |          enable_sagemaker_metrics (bool): enable SageMaker Metrics Time\n",
      " |              Series. For more information see:\n",
      " |              https://docs.aws.amazon.com/sagemaker/latest/dg/API_AlgorithmSpecification.html#SageMaker-Type-AlgorithmSpecification-EnableSageMakerMetricsTimeSeries\n",
      " |              (default: ``None``).\n",
      " |          **kwargs: Additional kwargs passed to the ``EstimatorBase``\n",
      " |              constructor.\n",
      " |      \n",
      " |      .. tip::\n",
      " |      \n",
      " |          You can find additional parameters for initializing this class at\n",
      " |          :class:`~sagemaker.estimator.EstimatorBase`.\n",
      " |  \n",
      " |  hyperparameters(self)\n",
      " |      Return the hyperparameters as a dictionary to use for training.\n",
      " |      \n",
      " |      The :meth:`~sagemaker.estimator.EstimatorBase.fit` method, which\n",
      " |      trains the model, calls this method to find the hyperparameters.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict[str, str]: The hyperparameters.\n",
      " |  \n",
      " |  training_image_uri(self)\n",
      " |      Return the Docker image to use for training.\n",
      " |      \n",
      " |      The :meth:`~sagemaker.estimator.EstimatorBase.fit` method, which does\n",
      " |      the model training, calls this method to find the image to use for model\n",
      " |      training.\n",
      " |      \n",
      " |      Returns:\n",
      " |          str: The URI of the Docker image.\n",
      " |  \n",
      " |  transformer(self, instance_count, instance_type, strategy=None, assemble_with=None, output_path=None, output_kms_key=None, accept=None, env=None, max_concurrent_transforms=None, max_payload=None, tags=None, role=None, model_server_workers=None, volume_kms_key=None, entry_point=None, vpc_config_override='VPC_CONFIG_DEFAULT', enable_network_isolation=None, model_name=None)\n",
      " |      Return a ``Transformer`` that uses a SageMaker Model based on the training job.\n",
      " |      \n",
      " |      It reuses the SageMaker Session and base job name used by\n",
      " |      the Estimator.\n",
      " |      \n",
      " |      Args:\n",
      " |          instance_count (int): Number of EC2 instances to use.\n",
      " |          instance_type (str): Type of EC2 instance to use, for example,\n",
      " |              'ml.c4.xlarge'.\n",
      " |          strategy (str): The strategy used to decide how to batch records in\n",
      " |              a single request (default: None). Valid values: 'MultiRecord'\n",
      " |              and 'SingleRecord'.\n",
      " |          assemble_with (str): How the output is assembled (default: None).\n",
      " |              Valid values: 'Line' or 'None'.\n",
      " |          output_path (str): S3 location for saving the transform result. If\n",
      " |              not specified, results are stored to a default bucket.\n",
      " |          output_kms_key (str): Optional. KMS key ID for encrypting the\n",
      " |              transform output (default: None).\n",
      " |          accept (str): The accept header passed by the client to\n",
      " |              the inference endpoint. If it is supported by the endpoint,\n",
      " |              it will be the format of the batch transform output.\n",
      " |          env (dict): Environment variables to be set for use during the\n",
      " |              transform job (default: None).\n",
      " |          max_concurrent_transforms (int): The maximum number of HTTP requests\n",
      " |              to be made to each individual transform container at one time.\n",
      " |          max_payload (int): Maximum size of the payload in a single HTTP\n",
      " |              request to the container in MB.\n",
      " |          tags (list[dict]): List of tags for labeling a transform job. If\n",
      " |              none specified, then the tags used for the training job are used\n",
      " |              for the transform job.\n",
      " |          role (str): The ``ExecutionRoleArn`` IAM Role ARN for the ``Model``,\n",
      " |              which is also used during transform jobs. If not specified, the\n",
      " |              role from the Estimator will be used.\n",
      " |          model_server_workers (int): Optional. The number of worker processes\n",
      " |              used by the inference server. If None, server will use one\n",
      " |              worker per vCPU.\n",
      " |          volume_kms_key (str): Optional. KMS key ID for encrypting the volume\n",
      " |              attached to the ML compute instance (default: None).\n",
      " |          entry_point (str): Path (absolute or relative) to the local Python source file which\n",
      " |              should be executed as the entry point to training. If ``source_dir`` is specified,\n",
      " |              then ``entry_point`` must point to a file located at the root of ``source_dir``.\n",
      " |              If not specified, the training entry point is used.\n",
      " |          vpc_config_override (dict[str, list[str]]): Optional override for\n",
      " |              the VpcConfig set on the model.\n",
      " |              Default: use subnets and security groups from this Estimator.\n",
      " |      \n",
      " |              * 'Subnets' (list[str]): List of subnet ids.\n",
      " |              * 'SecurityGroupIds' (list[str]): List of security group ids.\n",
      " |      \n",
      " |          enable_network_isolation (bool): Specifies whether container will\n",
      " |              run in network isolation mode. Network isolation mode restricts\n",
      " |              the container access to outside networks (such as the internet).\n",
      " |              The container does not make any inbound or outbound network\n",
      " |              calls. If True, a channel named \"code\" will be created for any\n",
      " |              user entry script for inference. Also known as Internet-free mode.\n",
      " |              If not specified, this setting is taken from the estimator's\n",
      " |              current configuration.\n",
      " |          model_name (str): Name to use for creating an Amazon SageMaker\n",
      " |              model. If not specified, the estimator generates a default job name\n",
      " |              based on the training image name and current timestamp.\n",
      " |      \n",
      " |      Returns:\n",
      " |          sagemaker.transformer.Transformer: a ``Transformer`` object that can be used to start a\n",
      " |              SageMaker Batch Transform job.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  attach(training_job_name, sagemaker_session=None, model_channel_name='model') from abc.ABCMeta\n",
      " |      Attach to an existing training job.\n",
      " |      \n",
      " |      Create an Estimator bound to an existing training job, each subclass\n",
      " |      is responsible to implement\n",
      " |      ``_prepare_init_params_from_job_description()`` as this method delegates\n",
      " |      the actual conversion of a training job description to the arguments\n",
      " |      that the class constructor expects. After attaching, if the training job\n",
      " |      has a Complete status, it can be ``deploy()`` ed to create a SageMaker\n",
      " |      Endpoint and return a ``Predictor``.\n",
      " |      \n",
      " |      If the training job is in progress, attach will block until the training job\n",
      " |      completes, but logs of the training job will not display. To see the logs\n",
      " |      content, please call ``logs()``\n",
      " |      \n",
      " |      Examples:\n",
      " |          >>> my_estimator.fit(wait=False)\n",
      " |          >>> training_job_name = my_estimator.latest_training_job.name\n",
      " |          Later on:\n",
      " |          >>> attached_estimator = Estimator.attach(training_job_name)\n",
      " |          >>> attached_estimator.logs()\n",
      " |          >>> attached_estimator.deploy()\n",
      " |      \n",
      " |      Args:\n",
      " |          training_job_name (str): The name of the training job to attach to.\n",
      " |          sagemaker_session (sagemaker.session.Session): Session object which\n",
      " |              manages interactions with Amazon SageMaker APIs and any other\n",
      " |              AWS services needed. If not specified, the estimator creates one\n",
      " |              using the default AWS configuration chain.\n",
      " |          model_channel_name (str): Name of the channel where pre-trained\n",
      " |              model data will be downloaded (default: 'model'). If no channel\n",
      " |              with the same name exists in the training job, this option will\n",
      " |              be ignored.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Instance of the calling ``Estimator`` Class with the attached\n",
      " |          training job.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  CONTAINER_CODE_CHANNEL_SOURCEDIR_PATH = '/opt/ml/input/data/code/sourc...\n",
      " |  \n",
      " |  INSTANCE_TYPE = 'sagemaker_instance_type'\n",
      " |  \n",
      " |  LAUNCH_MPI_ENV_NAME = 'sagemaker_mpi_enabled'\n",
      " |  \n",
      " |  LAUNCH_PS_ENV_NAME = 'sagemaker_parameter_server_enabled'\n",
      " |  \n",
      " |  LAUNCH_SM_DDP_ENV_NAME = 'sagemaker_distributed_dataparallel_enabled'\n",
      " |  \n",
      " |  MPI_CUSTOM_MPI_OPTIONS = 'sagemaker_mpi_custom_mpi_options'\n",
      " |  \n",
      " |  MPI_NUM_PROCESSES_PER_HOST = 'sagemaker_mpi_num_of_processes_per_host'\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset({'create_model'})\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from EstimatorBase:\n",
      " |  \n",
      " |  compile_model(self, target_instance_family, input_shape, output_path, framework=None, framework_version=None, compile_max_run=900, tags=None, target_platform_os=None, target_platform_arch=None, target_platform_accelerator=None, compiler_options=None, **kwargs)\n",
      " |      Compile a Neo model using the input model.\n",
      " |      \n",
      " |      Args:\n",
      " |          target_instance_family (str): Identifies the device that you want to\n",
      " |              run your model after compilation, for example: ml_c5. For allowed\n",
      " |              strings see\n",
      " |              https://docs.aws.amazon.com/sagemaker/latest/dg/API_OutputConfig.html.\n",
      " |          input_shape (dict): Specifies the name and shape of the expected\n",
      " |              inputs for your trained model in json dictionary form, for\n",
      " |              example: {'data':[1,3,1024,1024]}, or {'var1': [1,1,28,28],\n",
      " |              'var2':[1,1,28,28]}\n",
      " |          output_path (str): Specifies where to store the compiled model\n",
      " |          framework (str): The framework that is used to train the original\n",
      " |              model. Allowed values: 'mxnet', 'tensorflow', 'keras', 'pytorch',\n",
      " |              'onnx', 'xgboost'\n",
      " |          framework_version (str): The version of the framework\n",
      " |          compile_max_run (int): Timeout in seconds for compilation (default:\n",
      " |              3 * 60). After this amount of time Amazon SageMaker Neo\n",
      " |              terminates the compilation job regardless of its current status.\n",
      " |          tags (list[dict]): List of tags for labeling a compilation job. For\n",
      " |              more, see\n",
      " |              https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.\n",
      " |          target_platform_os (str): Target Platform OS, for example: 'LINUX'.\n",
      " |              For allowed strings see\n",
      " |              https://docs.aws.amazon.com/sagemaker/latest/dg/API_OutputConfig.html.\n",
      " |              It can be used instead of target_instance_family.\n",
      " |          target_platform_arch (str): Target Platform Architecture, for example: 'X86_64'.\n",
      " |              For allowed strings see\n",
      " |              https://docs.aws.amazon.com/sagemaker/latest/dg/API_OutputConfig.html.\n",
      " |              It can be used instead of target_instance_family.\n",
      " |          target_platform_accelerator (str, optional): Target Platform Accelerator,\n",
      " |              for example: 'NVIDIA'. For allowed strings see\n",
      " |              https://docs.aws.amazon.com/sagemaker/latest/dg/API_OutputConfig.html.\n",
      " |              It can be used instead of target_instance_family.\n",
      " |          compiler_options (dict, optional): Additional parameters for compiler.\n",
      " |              Compiler Options are TargetPlatform / target_instance_family specific. See\n",
      " |              https://docs.aws.amazon.com/sagemaker/latest/dg/API_OutputConfig.html for details.\n",
      " |          **kwargs: Passed to invocation of ``create_model()``.\n",
      " |              Implementations may customize ``create_model()`` to accept\n",
      " |              ``**kwargs`` to customize model creation during deploy. For\n",
      " |              more, see the implementation docs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          sagemaker.model.Model: A SageMaker ``Model`` object. See\n",
      " |          :func:`~sagemaker.model.Model` for full details.\n",
      " |  \n",
      " |  create_model(self, **kwargs)\n",
      " |      Create a SageMaker ``Model`` object that can be deployed to an ``Endpoint``.\n",
      " |      \n",
      " |      Args:\n",
      " |          **kwargs: Keyword arguments used by the implemented method for\n",
      " |              creating the ``Model``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          sagemaker.model.Model: A SageMaker ``Model`` object. See\n",
      " |          :func:`~sagemaker.model.Model` for full details.\n",
      " |  \n",
      " |  delete_endpoint = func(*args, **kwargs)\n",
      " |  \n",
      " |  deploy(self, initial_instance_count, instance_type, serializer=None, deserializer=None, accelerator_type=None, endpoint_name=None, use_compiled_model=False, wait=True, model_name=None, kms_key=None, data_capture_config=None, tags=None, **kwargs)\n",
      " |      Deploy the trained model to an Amazon SageMaker endpoint.\n",
      " |      \n",
      " |       And then return ``sagemaker.Predictor`` object.\n",
      " |      \n",
      " |      More information:\n",
      " |      http://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html\n",
      " |      \n",
      " |      Args:\n",
      " |          initial_instance_count (int): Minimum number of EC2 instances to\n",
      " |              deploy to an endpoint for prediction.\n",
      " |          instance_type (str): Type of EC2 instance to deploy to an endpoint\n",
      " |              for prediction, for example, 'ml.c4.xlarge'.\n",
      " |          serializer (:class:`~sagemaker.serializers.BaseSerializer`): A\n",
      " |              serializer object, used to encode data for an inference endpoint\n",
      " |              (default: None). If ``serializer`` is not None, then\n",
      " |              ``serializer`` will override the default serializer. The\n",
      " |              default serializer is set by the ``predictor_cls``.\n",
      " |          deserializer (:class:`~sagemaker.deserializers.BaseDeserializer`): A\n",
      " |              deserializer object, used to decode data from an inference\n",
      " |              endpoint (default: None). If ``deserializer`` is not None, then\n",
      " |              ``deserializer`` will override the default deserializer. The\n",
      " |              default deserializer is set by the ``predictor_cls``.\n",
      " |          accelerator_type (str): Type of Elastic Inference accelerator to\n",
      " |              attach to an endpoint for model loading and inference, for\n",
      " |              example, 'ml.eia1.medium'. If not specified, no Elastic\n",
      " |              Inference accelerator will be attached to the endpoint. For more\n",
      " |              information:\n",
      " |              https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html\n",
      " |          endpoint_name (str): Name to use for creating an Amazon SageMaker\n",
      " |              endpoint. If not specified, the name of the training job is\n",
      " |              used.\n",
      " |          use_compiled_model (bool): Flag to select whether to use compiled\n",
      " |              (optimized) model. Default: False.\n",
      " |          wait (bool): Whether the call should wait until the deployment of\n",
      " |              model completes (default: True).\n",
      " |          model_name (str): Name to use for creating an Amazon SageMaker\n",
      " |              model. If not specified, the estimator generates a default job name\n",
      " |              based on the training image name and current timestamp.\n",
      " |          kms_key (str): The ARN of the KMS key that is used to encrypt the\n",
      " |              data on the storage volume attached to the instance hosting the\n",
      " |              endpoint.\n",
      " |          data_capture_config (sagemaker.model_monitor.DataCaptureConfig): Specifies\n",
      " |              configuration related to Endpoint data capture for use with\n",
      " |              Amazon SageMaker Model Monitoring. Default: None.\n",
      " |          tags(List[dict[str, str]]): Optional. The list of tags to attach to this specific\n",
      " |              endpoint. Example:\n",
      " |              >>> tags = [{'Key': 'tagname', 'Value': 'tagvalue'}]\n",
      " |              For more information about tags, see\n",
      " |              https://boto3.amazonaws.com/v1/documentation                /api/latest/reference/services/sagemaker.html#SageMaker.Client.add_tags\n",
      " |          **kwargs: Passed to invocation of ``create_model()``.\n",
      " |              Implementations may customize ``create_model()`` to accept\n",
      " |              ``**kwargs`` to customize model creation during deploy.\n",
      " |              For more, see the implementation docs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          sagemaker.predictor.Predictor: A predictor that provides a ``predict()`` method,\n",
      " |              which can be used to send requests to the Amazon SageMaker\n",
      " |              endpoint and obtain inferences.\n",
      " |  \n",
      " |  disable_profiling(self)\n",
      " |      Update the current training job in progress to disable profiling.\n",
      " |      \n",
      " |      Debugger stops collecting the system and framework metrics\n",
      " |      and turns off the Debugger built-in monitoring and profiling rules.\n",
      " |  \n",
      " |  enable_default_profiling(self)\n",
      " |      Update training job to enable Debugger monitoring.\n",
      " |      \n",
      " |      This method enables Debugger monitoring with\n",
      " |      the default ``profiler_config`` parameter to collect system\n",
      " |      metrics and the default built-in ``profiler_report`` rule.\n",
      " |      Framework metrics won't be saved.\n",
      " |      To update training job to emit framework metrics, you can use\n",
      " |      :class:`~sagemaker.estimator.Estimator.update_profiler`\n",
      " |      method and specify the framework metrics you want to enable.\n",
      " |      \n",
      " |      This method is callable when the training job is in progress while\n",
      " |      Debugger monitoring is disabled.\n",
      " |  \n",
      " |  enable_network_isolation(self)\n",
      " |      Return True if this Estimator will need network isolation to run.\n",
      " |      \n",
      " |      Returns:\n",
      " |          bool: Whether this Estimator needs network isolation or not.\n",
      " |  \n",
      " |  fit(self, inputs=None, wait=True, logs='All', job_name=None, experiment_config=None)\n",
      " |      Train a model using the input training dataset.\n",
      " |      \n",
      " |      The API calls the Amazon SageMaker CreateTrainingJob API to start\n",
      " |      model training. The API uses configuration you provided to create the\n",
      " |      estimator and the specified input training data to send the\n",
      " |      CreatingTrainingJob request to Amazon SageMaker.\n",
      " |      \n",
      " |      This is a synchronous operation. After the model training\n",
      " |      successfully completes, you can call the ``deploy()`` method to host the\n",
      " |      model using the Amazon SageMaker hosting services.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs (str or dict or sagemaker.inputs.TrainingInput): Information\n",
      " |              about the training data. This can be one of three types:\n",
      " |      \n",
      " |              * (str) the S3 location where training data is saved, or a file:// path in\n",
      " |                  local mode.\n",
      " |              * (dict[str, str] or dict[str, sagemaker.inputs.TrainingInput]) If using multiple\n",
      " |                  channels for training data, you can specify a dict mapping channel names to\n",
      " |                  strings or :func:`~sagemaker.inputs.TrainingInput` objects.\n",
      " |              * (sagemaker.inputs.TrainingInput) - channel configuration for S3 data sources\n",
      " |                  that can provide additional information as well as the path to the training\n",
      " |                  dataset.\n",
      " |                  See :func:`sagemaker.inputs.TrainingInput` for full details.\n",
      " |              * (sagemaker.session.FileSystemInput) - channel configuration for\n",
      " |                  a file system data source that can provide additional information as well as\n",
      " |                  the path to the training dataset.\n",
      " |      \n",
      " |          wait (bool): Whether the call should wait until the job completes (default: True).\n",
      " |          logs ([str]): A list of strings specifying which logs to print. Acceptable\n",
      " |              strings are \"All\", \"None\", \"Training\", or \"Rules\". To maintain backwards\n",
      " |              compatibility, boolean values are also accepted and converted to strings.\n",
      " |              Only meaningful when wait is True.\n",
      " |          job_name (str): Training job name. If not specified, the estimator generates\n",
      " |              a default job name based on the training image name and current timestamp.\n",
      " |          experiment_config (dict[str, str]): Experiment management configuration.\n",
      " |              Dictionary contains three optional keys,\n",
      " |              'ExperimentName', 'TrialName', and 'TrialComponentDisplayName'.\n",
      " |  \n",
      " |  get_vpc_config(self, vpc_config_override='VPC_CONFIG_DEFAULT')\n",
      " |      Returns VpcConfig dict either from this Estimator's subnets and security groups.\n",
      " |      \n",
      " |      Or else validate and return an optional override value.\n",
      " |      \n",
      " |      Args:\n",
      " |          vpc_config_override:\n",
      " |  \n",
      " |  latest_job_debugger_artifacts_path(self)\n",
      " |      Gets the path to the DebuggerHookConfig output artifacts.\n",
      " |      \n",
      " |      Returns:\n",
      " |          str: An S3 path to the output artifacts.\n",
      " |  \n",
      " |  latest_job_profiler_artifacts_path(self)\n",
      " |      Gets the path to the profiling output artifacts.\n",
      " |      \n",
      " |      Returns:\n",
      " |          str: An S3 path to the output artifacts.\n",
      " |  \n",
      " |  latest_job_tensorboard_artifacts_path(self)\n",
      " |      Gets the path to the TensorBoardOutputConfig output artifacts.\n",
      " |      \n",
      " |      Returns:\n",
      " |          str: An S3 path to the output artifacts.\n",
      " |  \n",
      " |  logs(self)\n",
      " |      Display the logs for Estimator's training job.\n",
      " |      \n",
      " |      If the output is a tty or a Jupyter cell, it will be color-coded based\n",
      " |      on which instance the log entry is from.\n",
      " |  \n",
      " |  prepare_workflow_for_training(self, job_name=None)\n",
      " |      Calls _prepare_for_training. Used when setting up a workflow.\n",
      " |      \n",
      " |      Args:\n",
      " |          job_name (str): Name of the training job to be created. If not\n",
      " |              specified, one is generated, using the base name given to the\n",
      " |              constructor if applicable.\n",
      " |  \n",
      " |  register(self, content_types, response_types, inference_instances, transform_instances, image_uri=None, model_package_name=None, model_package_group_name=None, model_metrics=None, metadata_properties=None, marketplace_cert=False, approval_status=None, description=None, compile_model_family=None, model_name=None, **kwargs)\n",
      " |      Creates a model package for creating SageMaker models or listing on Marketplace.\n",
      " |      \n",
      " |      Args:\n",
      " |          content_types (list): The supported MIME types for the input data.\n",
      " |          response_types (list): The supported MIME types for the output data.\n",
      " |          inference_instances (list): A list of the instance types that are used to\n",
      " |              generate inferences in real-time.\n",
      " |          transform_instances (list): A list of the instance types on which a transformation\n",
      " |              job can be run or on which an endpoint can be deployed.\n",
      " |          image_uri (str): The container image uri for Model Package, if not specified,\n",
      " |              Estimator's training container image will be used (default: None).\n",
      " |          model_package_name (str): Model Package name, exclusive to `model_package_group_name`,\n",
      " |              using `model_package_name` makes the Model Package un-versioned (default: None).\n",
      " |          model_package_group_name (str): Model Package Group name, exclusive to\n",
      " |              `model_package_name`, using `model_package_group_name` makes the Model Package\n",
      " |              versioned (default: None).\n",
      " |          model_metrics (ModelMetrics): ModelMetrics object (default: None).\n",
      " |          metadata_properties (MetadataProperties): MetadataProperties (default: None).\n",
      " |          marketplace_cert (bool): A boolean value indicating if the Model Package is certified\n",
      " |              for AWS Marketplace (default: False).\n",
      " |          approval_status (str): Model Approval Status, values can be \"Approved\", \"Rejected\",\n",
      " |              or \"PendingManualApproval\" (default: \"PendingManualApproval\").\n",
      " |          description (str): Model Package description (default: None).\n",
      " |          compile_model_family (str): Instance family for compiled model, if specified, a compiled\n",
      " |              model will be used (default: None).\n",
      " |          model_name (str): User defined model name (default: None).\n",
      " |          **kwargs: Passed to invocation of ``create_model()``. Implementations may customize\n",
      " |              ``create_model()`` to accept ``**kwargs`` to customize model creation during\n",
      " |              deploy. For more, see the implementation docs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          str: A string of SageMaker Model Package ARN.\n",
      " |  \n",
      " |  update_profiler(self, rules=None, system_monitor_interval_millis=None, s3_output_path=None, framework_profile_params=None, disable_framework_metrics=False)\n",
      " |      Update training jobs to enable profiling.\n",
      " |      \n",
      " |      This method updates the ``profiler_config`` parameter\n",
      " |      and initiates Debugger built-in rules for profiling.\n",
      " |      \n",
      " |      Args:\n",
      " |          rules (list[:class:`~sagemaker.debugger.ProfilerRule`]): A list of\n",
      " |              :class:`~sagemaker.debugger.ProfilerRule` objects to define\n",
      " |              rules for continuous analysis with SageMaker Debugger. Currently, you can\n",
      " |              only add new profiler rules during the training job. (default: ``None``)\n",
      " |          s3_output_path (str): The location in S3 to store the output. If profiler is enabled\n",
      " |              once, s3_output_path cannot be changed. (default: ``None``)\n",
      " |          system_monitor_interval_millis (int): How often profiling system metrics are\n",
      " |              collected; Unit: Milliseconds (default: ``None``)\n",
      " |          framework_profile_params (:class:`~sagemaker.debugger.FrameworkProfile`):\n",
      " |              A parameter object for framework metrics profiling. Configure it using\n",
      " |              the :class:`~sagemaker.debugger.FrameworkProfile` class.\n",
      " |              To use the default framework profile parameters, pass ``FrameworkProfile()``.\n",
      " |              For more information about the default values,\n",
      " |              see :class:`~sagemaker.debugger.FrameworkProfile`. (default: ``None``)\n",
      " |          disable_framework_metrics (bool): Specify whether to disable all the framework metrics.\n",
      " |              This won't update system metrics and the Debugger built-in rules for monitoring.\n",
      " |              To stop both monitoring and profiling,\n",
      " |              use the :class:`~sagemaker.estimator.Estimator.desable_profiling`\n",
      " |              method. (default: ``False``)\n",
      " |      \n",
      " |      .. attention::\n",
      " |      \n",
      " |          Updating the profiling configuration for TensorFlow dataloader profiling\n",
      " |          is currently not available. If you started a TensorFlow training job only with\n",
      " |          monitoring and want to enable profiling while the training job is running,\n",
      " |          the dataloader profiling cannot be updated.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from EstimatorBase:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  model_data\n",
      " |      str: The model location in S3. Only set if Estimator has been ``fit()``.\n",
      " |  \n",
      " |  training_job_analytics\n",
      " |      Return a ``TrainingJobAnalytics`` object for the current training job.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sage.estimator.Framework)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class TensorFlow in module sagemaker.tensorflow.estimator:\n",
      "\n",
      "class TensorFlow(sagemaker.estimator.Framework)\n",
      " |  Handle end-to-end training and deployment of user-provided TensorFlow code.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TensorFlow\n",
      " |      sagemaker.estimator.Framework\n",
      " |      sagemaker.estimator.EstimatorBase\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, py_version=None, framework_version=None, model_dir=None, image_uri=None, distribution=None, **kwargs)\n",
      " |      Initialize a ``TensorFlow`` estimator.\n",
      " |      \n",
      " |      Args:\n",
      " |          py_version (str): Python version you want to use for executing your model training\n",
      " |              code. Defaults to ``None``. Required unless ``image_uri`` is provided.\n",
      " |          framework_version (str): TensorFlow version you want to use for executing your model\n",
      " |              training code. Defaults to ``None``. Required unless ``image_uri`` is provided.\n",
      " |              List of supported versions:\n",
      " |              https://github.com/aws/sagemaker-python-sdk#tensorflow-sagemaker-estimators.\n",
      " |          model_dir (str): S3 location where the checkpoint data and models can be exported to\n",
      " |              during training (default: None). It will be passed in the training script as one of\n",
      " |              the command line arguments. If not specified, one is provided based on\n",
      " |              your training configuration:\n",
      " |      \n",
      " |              * *distributed training with SMDistributed or MPI with Horovod* - ``/opt/ml/model``\n",
      " |              * *single-machine training or distributed training without MPI* -                     ``s3://{output_path}/model``\n",
      " |              * *Local Mode with local sources (file:// instead of s3://)* -                     ``/opt/ml/shared/model``\n",
      " |      \n",
      " |              To disable having ``model_dir`` passed to your training script,\n",
      " |              set ``model_dir=False``.\n",
      " |          image_uri (str): If specified, the estimator will use this image for training and\n",
      " |              hosting, instead of selecting the appropriate SageMaker official image based on\n",
      " |              framework_version and py_version. It can be an ECR url or dockerhub image and tag.\n",
      " |      \n",
      " |              Examples:\n",
      " |                  123.dkr.ecr.us-west-2.amazonaws.com/my-custom-image:1.0\n",
      " |                  custom-image:latest.\n",
      " |      \n",
      " |              If ``framework_version`` or ``py_version`` are ``None``, then\n",
      " |              ``image_uri`` is required. If also ``None``, then a ``ValueError``\n",
      " |              will be raised.\n",
      " |          distribution (dict): A dictionary with information on how to run distributed training\n",
      " |              (default: None). Currently, the following are supported:\n",
      " |              distributed training with parameter servers, SageMaker Distributed (SMD) Data\n",
      " |              and Model Parallelism, and MPI. SMD Model Parallelism can only be used with MPI.\n",
      " |              To enable parameter server use the following setup:\n",
      " |      \n",
      " |              .. code:: python\n",
      " |      \n",
      " |                  {\n",
      " |                      \"parameter_server\": {\n",
      " |                          \"enabled\": True\n",
      " |                      }\n",
      " |                  }\n",
      " |      \n",
      " |              To enable MPI:\n",
      " |      \n",
      " |              .. code:: python\n",
      " |      \n",
      " |                  {\n",
      " |                      \"mpi\": {\n",
      " |                          \"enabled\": True\n",
      " |                      }\n",
      " |                  }\n",
      " |      \n",
      " |              To enable SMDistributed Data Parallel or Model Parallel:\n",
      " |      \n",
      " |              .. code:: python\n",
      " |      \n",
      " |                  {\n",
      " |                      \"smdistributed\": {\n",
      " |                          \"dataparallel\": {\n",
      " |                              \"enabled\": True\n",
      " |                          },\n",
      " |                          \"modelparallel\": {\n",
      " |                              \"enabled\": True,\n",
      " |                              \"parameters\": {}\n",
      " |                          }\n",
      " |                      }\n",
      " |                  }\n",
      " |      \n",
      " |          **kwargs: Additional kwargs passed to the Framework constructor.\n",
      " |      \n",
      " |      .. tip::\n",
      " |      \n",
      " |          You can find additional parameters for initializing this class at\n",
      " |          :class:`~sagemaker.estimator.Framework` and\n",
      " |          :class:`~sagemaker.estimator.EstimatorBase`.\n",
      " |  \n",
      " |  create_model(self, role=None, vpc_config_override='VPC_CONFIG_DEFAULT', entry_point=None, source_dir=None, dependencies=None, **kwargs)\n",
      " |      Creates ``TensorFlowModel`` object to be used for creating SageMaker model entities.\n",
      " |      \n",
      " |      This can be done by deploying it to a SageMaker endpoint,\n",
      " |      or starting SageMaker Batch Transform jobs.\n",
      " |      \n",
      " |      Args:\n",
      " |          role (str): The ``TensorFlowModel``, which is also used during transform jobs.\n",
      " |              If not specified, the role from the Estimator is used.\n",
      " |          vpc_config_override (dict[str, list[str]]): Optional override for VpcConfig set on the\n",
      " |              model. Default: use subnets and security groups from this Estimator.\n",
      " |      \n",
      " |              * 'Subnets' (list[str]): List of subnet ids.\n",
      " |              * 'SecurityGroupIds' (list[str]): List of security group ids.\n",
      " |      \n",
      " |          entry_point (str): Path (absolute or relative) to the local Python source file which\n",
      " |              should be executed as the entry point to training. If ``source_dir`` is specified,\n",
      " |              then ``entry_point`` must point to a file located at the root of ``source_dir``.\n",
      " |              If not specified and ``endpoint_type`` is 'tensorflow-serving',\n",
      " |              no entry point is used. If ``endpoint_type`` is also ``None``,\n",
      " |              then the training entry point is used.\n",
      " |          source_dir (str): Path (absolute or relative or an S3 URI) to a directory with any other\n",
      " |              serving source code dependencies aside from the entry point file (default: None).\n",
      " |          dependencies (list[str]): A list of paths to directories (absolute or relative) with\n",
      " |              any additional libraries that will be exported to the container (default: None).\n",
      " |          **kwargs: Additional kwargs passed to\n",
      " |              :class:`~sagemaker.tensorflow.model.TensorFlowModel`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          sagemaker.tensorflow.model.TensorFlowModel: A ``TensorFlowModel`` object.\n",
      " |              See :class:`~sagemaker.tensorflow.model.TensorFlowModel` for full details.\n",
      " |  \n",
      " |  hyperparameters(self)\n",
      " |      Return hyperparameters used by your custom TensorFlow code during model training.\n",
      " |  \n",
      " |  transformer(self, instance_count, instance_type, strategy=None, assemble_with=None, output_path=None, output_kms_key=None, accept=None, env=None, max_concurrent_transforms=None, max_payload=None, tags=None, role=None, volume_kms_key=None, entry_point=None, vpc_config_override='VPC_CONFIG_DEFAULT', enable_network_isolation=None, model_name=None)\n",
      " |      Return a ``Transformer`` that uses a SageMaker Model based on the training job.\n",
      " |      \n",
      " |      It reuses the SageMaker Session and base job name used by the Estimator.\n",
      " |      \n",
      " |      Args:\n",
      " |          instance_count (int): Number of EC2 instances to use.\n",
      " |          instance_type (str): Type of EC2 instance to use, for example, 'ml.c4.xlarge'.\n",
      " |          strategy (str): The strategy used to decide how to batch records in a single request\n",
      " |              (default: None). Valid values: 'MultiRecord' and 'SingleRecord'.\n",
      " |          assemble_with (str): How the output is assembled (default: None). Valid values: 'Line'\n",
      " |              or 'None'.\n",
      " |          output_path (str): S3 location for saving the transform result. If not specified,\n",
      " |              results are stored to a default bucket.\n",
      " |          output_kms_key (str): Optional. KMS key ID for encrypting the transform output\n",
      " |              (default: None).\n",
      " |          accept (str): The accept header passed by the client to\n",
      " |              the inference endpoint. If it is supported by the endpoint,\n",
      " |              it will be the format of the batch transform output.\n",
      " |          env (dict): Environment variables to be set for use during the transform job\n",
      " |              (default: None).\n",
      " |          max_concurrent_transforms (int): The maximum number of HTTP requests to be made to\n",
      " |              each individual transform container at one time.\n",
      " |          max_payload (int): Maximum size of the payload in a single HTTP request to the\n",
      " |              container in MB.\n",
      " |          tags (list[dict]): List of tags for labeling a transform job. If none specified, then\n",
      " |              the tags used for the training job are used for the transform job.\n",
      " |          role (str): The IAM Role ARN for the ``TensorFlowModel``, which is also used\n",
      " |              during transform jobs. If not specified, the role from the Estimator is used.\n",
      " |          volume_kms_key (str): Optional. KMS key ID for encrypting the volume attached to the ML\n",
      " |              compute instance (default: None).\n",
      " |          entry_point (str): Path (absolute or relative) to the local Python source file which\n",
      " |              should be executed as the entry point to training. If ``source_dir`` is specified,\n",
      " |              then ``entry_point`` must point to a file located at the root of ``source_dir``.\n",
      " |              If not specified and ``endpoint_type`` is 'tensorflow-serving',\n",
      " |              no entry point is used. If ``endpoint_type`` is also ``None``,\n",
      " |              then the training entry point is used.\n",
      " |          vpc_config_override (dict[str, list[str]]): Optional override for\n",
      " |              the VpcConfig set on the model.\n",
      " |              Default: use subnets and security groups from this Estimator.\n",
      " |      \n",
      " |              * 'Subnets' (list[str]): List of subnet ids.\n",
      " |              * 'SecurityGroupIds' (list[str]): List of security group ids.\n",
      " |      \n",
      " |          enable_network_isolation (bool): Specifies whether container will\n",
      " |              run in network isolation mode. Network isolation mode restricts\n",
      " |              the container access to outside networks (such as the internet).\n",
      " |              The container does not make any inbound or outbound network\n",
      " |              calls. If True, a channel named \"code\" will be created for any\n",
      " |              user entry script for inference. Also known as Internet-free mode.\n",
      " |              If not specified, this setting is taken from the estimator's\n",
      " |              current configuration.\n",
      " |          model_name (str): Name to use for creating an Amazon SageMaker\n",
      " |              model. If not specified, the estimator generates a default job name\n",
      " |              based on the training image name and current timestamp.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sagemaker.estimator.Framework:\n",
      " |  \n",
      " |  training_image_uri(self)\n",
      " |      Return the Docker image to use for training.\n",
      " |      \n",
      " |      The :meth:`~sagemaker.estimator.EstimatorBase.fit` method, which does\n",
      " |      the model training, calls this method to find the image to use for model\n",
      " |      training.\n",
      " |      \n",
      " |      Returns:\n",
      " |          str: The URI of the Docker image.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from sagemaker.estimator.Framework:\n",
      " |  \n",
      " |  attach(training_job_name, sagemaker_session=None, model_channel_name='model') from abc.ABCMeta\n",
      " |      Attach to an existing training job.\n",
      " |      \n",
      " |      Create an Estimator bound to an existing training job, each subclass\n",
      " |      is responsible to implement\n",
      " |      ``_prepare_init_params_from_job_description()`` as this method delegates\n",
      " |      the actual conversion of a training job description to the arguments\n",
      " |      that the class constructor expects. After attaching, if the training job\n",
      " |      has a Complete status, it can be ``deploy()`` ed to create a SageMaker\n",
      " |      Endpoint and return a ``Predictor``.\n",
      " |      \n",
      " |      If the training job is in progress, attach will block until the training job\n",
      " |      completes, but logs of the training job will not display. To see the logs\n",
      " |      content, please call ``logs()``\n",
      " |      \n",
      " |      Examples:\n",
      " |          >>> my_estimator.fit(wait=False)\n",
      " |          >>> training_job_name = my_estimator.latest_training_job.name\n",
      " |          Later on:\n",
      " |          >>> attached_estimator = Estimator.attach(training_job_name)\n",
      " |          >>> attached_estimator.logs()\n",
      " |          >>> attached_estimator.deploy()\n",
      " |      \n",
      " |      Args:\n",
      " |          training_job_name (str): The name of the training job to attach to.\n",
      " |          sagemaker_session (sagemaker.session.Session): Session object which\n",
      " |              manages interactions with Amazon SageMaker APIs and any other\n",
      " |              AWS services needed. If not specified, the estimator creates one\n",
      " |              using the default AWS configuration chain.\n",
      " |          model_channel_name (str): Name of the channel where pre-trained\n",
      " |              model data will be downloaded (default: 'model'). If no channel\n",
      " |              with the same name exists in the training job, this option will\n",
      " |              be ignored.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Instance of the calling ``Estimator`` Class with the attached\n",
      " |          training job.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from sagemaker.estimator.Framework:\n",
      " |  \n",
      " |  CONTAINER_CODE_CHANNEL_SOURCEDIR_PATH = '/opt/ml/input/data/code/sourc...\n",
      " |  \n",
      " |  INSTANCE_TYPE = 'sagemaker_instance_type'\n",
      " |  \n",
      " |  LAUNCH_MPI_ENV_NAME = 'sagemaker_mpi_enabled'\n",
      " |  \n",
      " |  LAUNCH_PS_ENV_NAME = 'sagemaker_parameter_server_enabled'\n",
      " |  \n",
      " |  LAUNCH_SM_DDP_ENV_NAME = 'sagemaker_distributed_dataparallel_enabled'\n",
      " |  \n",
      " |  MPI_CUSTOM_MPI_OPTIONS = 'sagemaker_mpi_custom_mpi_options'\n",
      " |  \n",
      " |  MPI_NUM_PROCESSES_PER_HOST = 'sagemaker_mpi_num_of_processes_per_host'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sagemaker.estimator.EstimatorBase:\n",
      " |  \n",
      " |  compile_model(self, target_instance_family, input_shape, output_path, framework=None, framework_version=None, compile_max_run=900, tags=None, target_platform_os=None, target_platform_arch=None, target_platform_accelerator=None, compiler_options=None, **kwargs)\n",
      " |      Compile a Neo model using the input model.\n",
      " |      \n",
      " |      Args:\n",
      " |          target_instance_family (str): Identifies the device that you want to\n",
      " |              run your model after compilation, for example: ml_c5. For allowed\n",
      " |              strings see\n",
      " |              https://docs.aws.amazon.com/sagemaker/latest/dg/API_OutputConfig.html.\n",
      " |          input_shape (dict): Specifies the name and shape of the expected\n",
      " |              inputs for your trained model in json dictionary form, for\n",
      " |              example: {'data':[1,3,1024,1024]}, or {'var1': [1,1,28,28],\n",
      " |              'var2':[1,1,28,28]}\n",
      " |          output_path (str): Specifies where to store the compiled model\n",
      " |          framework (str): The framework that is used to train the original\n",
      " |              model. Allowed values: 'mxnet', 'tensorflow', 'keras', 'pytorch',\n",
      " |              'onnx', 'xgboost'\n",
      " |          framework_version (str): The version of the framework\n",
      " |          compile_max_run (int): Timeout in seconds for compilation (default:\n",
      " |              3 * 60). After this amount of time Amazon SageMaker Neo\n",
      " |              terminates the compilation job regardless of its current status.\n",
      " |          tags (list[dict]): List of tags for labeling a compilation job. For\n",
      " |              more, see\n",
      " |              https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.\n",
      " |          target_platform_os (str): Target Platform OS, for example: 'LINUX'.\n",
      " |              For allowed strings see\n",
      " |              https://docs.aws.amazon.com/sagemaker/latest/dg/API_OutputConfig.html.\n",
      " |              It can be used instead of target_instance_family.\n",
      " |          target_platform_arch (str): Target Platform Architecture, for example: 'X86_64'.\n",
      " |              For allowed strings see\n",
      " |              https://docs.aws.amazon.com/sagemaker/latest/dg/API_OutputConfig.html.\n",
      " |              It can be used instead of target_instance_family.\n",
      " |          target_platform_accelerator (str, optional): Target Platform Accelerator,\n",
      " |              for example: 'NVIDIA'. For allowed strings see\n",
      " |              https://docs.aws.amazon.com/sagemaker/latest/dg/API_OutputConfig.html.\n",
      " |              It can be used instead of target_instance_family.\n",
      " |          compiler_options (dict, optional): Additional parameters for compiler.\n",
      " |              Compiler Options are TargetPlatform / target_instance_family specific. See\n",
      " |              https://docs.aws.amazon.com/sagemaker/latest/dg/API_OutputConfig.html for details.\n",
      " |          **kwargs: Passed to invocation of ``create_model()``.\n",
      " |              Implementations may customize ``create_model()`` to accept\n",
      " |              ``**kwargs`` to customize model creation during deploy. For\n",
      " |              more, see the implementation docs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          sagemaker.model.Model: A SageMaker ``Model`` object. See\n",
      " |          :func:`~sagemaker.model.Model` for full details.\n",
      " |  \n",
      " |  delete_endpoint = func(*args, **kwargs)\n",
      " |  \n",
      " |  deploy(self, initial_instance_count, instance_type, serializer=None, deserializer=None, accelerator_type=None, endpoint_name=None, use_compiled_model=False, wait=True, model_name=None, kms_key=None, data_capture_config=None, tags=None, **kwargs)\n",
      " |      Deploy the trained model to an Amazon SageMaker endpoint.\n",
      " |      \n",
      " |       And then return ``sagemaker.Predictor`` object.\n",
      " |      \n",
      " |      More information:\n",
      " |      http://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html\n",
      " |      \n",
      " |      Args:\n",
      " |          initial_instance_count (int): Minimum number of EC2 instances to\n",
      " |              deploy to an endpoint for prediction.\n",
      " |          instance_type (str): Type of EC2 instance to deploy to an endpoint\n",
      " |              for prediction, for example, 'ml.c4.xlarge'.\n",
      " |          serializer (:class:`~sagemaker.serializers.BaseSerializer`): A\n",
      " |              serializer object, used to encode data for an inference endpoint\n",
      " |              (default: None). If ``serializer`` is not None, then\n",
      " |              ``serializer`` will override the default serializer. The\n",
      " |              default serializer is set by the ``predictor_cls``.\n",
      " |          deserializer (:class:`~sagemaker.deserializers.BaseDeserializer`): A\n",
      " |              deserializer object, used to decode data from an inference\n",
      " |              endpoint (default: None). If ``deserializer`` is not None, then\n",
      " |              ``deserializer`` will override the default deserializer. The\n",
      " |              default deserializer is set by the ``predictor_cls``.\n",
      " |          accelerator_type (str): Type of Elastic Inference accelerator to\n",
      " |              attach to an endpoint for model loading and inference, for\n",
      " |              example, 'ml.eia1.medium'. If not specified, no Elastic\n",
      " |              Inference accelerator will be attached to the endpoint. For more\n",
      " |              information:\n",
      " |              https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html\n",
      " |          endpoint_name (str): Name to use for creating an Amazon SageMaker\n",
      " |              endpoint. If not specified, the name of the training job is\n",
      " |              used.\n",
      " |          use_compiled_model (bool): Flag to select whether to use compiled\n",
      " |              (optimized) model. Default: False.\n",
      " |          wait (bool): Whether the call should wait until the deployment of\n",
      " |              model completes (default: True).\n",
      " |          model_name (str): Name to use for creating an Amazon SageMaker\n",
      " |              model. If not specified, the estimator generates a default job name\n",
      " |              based on the training image name and current timestamp.\n",
      " |          kms_key (str): The ARN of the KMS key that is used to encrypt the\n",
      " |              data on the storage volume attached to the instance hosting the\n",
      " |              endpoint.\n",
      " |          data_capture_config (sagemaker.model_monitor.DataCaptureConfig): Specifies\n",
      " |              configuration related to Endpoint data capture for use with\n",
      " |              Amazon SageMaker Model Monitoring. Default: None.\n",
      " |          tags(List[dict[str, str]]): Optional. The list of tags to attach to this specific\n",
      " |              endpoint. Example:\n",
      " |              >>> tags = [{'Key': 'tagname', 'Value': 'tagvalue'}]\n",
      " |              For more information about tags, see\n",
      " |              https://boto3.amazonaws.com/v1/documentation                /api/latest/reference/services/sagemaker.html#SageMaker.Client.add_tags\n",
      " |          **kwargs: Passed to invocation of ``create_model()``.\n",
      " |              Implementations may customize ``create_model()`` to accept\n",
      " |              ``**kwargs`` to customize model creation during deploy.\n",
      " |              For more, see the implementation docs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          sagemaker.predictor.Predictor: A predictor that provides a ``predict()`` method,\n",
      " |              which can be used to send requests to the Amazon SageMaker\n",
      " |              endpoint and obtain inferences.\n",
      " |  \n",
      " |  disable_profiling(self)\n",
      " |      Update the current training job in progress to disable profiling.\n",
      " |      \n",
      " |      Debugger stops collecting the system and framework metrics\n",
      " |      and turns off the Debugger built-in monitoring and profiling rules.\n",
      " |  \n",
      " |  enable_default_profiling(self)\n",
      " |      Update training job to enable Debugger monitoring.\n",
      " |      \n",
      " |      This method enables Debugger monitoring with\n",
      " |      the default ``profiler_config`` parameter to collect system\n",
      " |      metrics and the default built-in ``profiler_report`` rule.\n",
      " |      Framework metrics won't be saved.\n",
      " |      To update training job to emit framework metrics, you can use\n",
      " |      :class:`~sagemaker.estimator.Estimator.update_profiler`\n",
      " |      method and specify the framework metrics you want to enable.\n",
      " |      \n",
      " |      This method is callable when the training job is in progress while\n",
      " |      Debugger monitoring is disabled.\n",
      " |  \n",
      " |  enable_network_isolation(self)\n",
      " |      Return True if this Estimator will need network isolation to run.\n",
      " |      \n",
      " |      Returns:\n",
      " |          bool: Whether this Estimator needs network isolation or not.\n",
      " |  \n",
      " |  fit(self, inputs=None, wait=True, logs='All', job_name=None, experiment_config=None)\n",
      " |      Train a model using the input training dataset.\n",
      " |      \n",
      " |      The API calls the Amazon SageMaker CreateTrainingJob API to start\n",
      " |      model training. The API uses configuration you provided to create the\n",
      " |      estimator and the specified input training data to send the\n",
      " |      CreatingTrainingJob request to Amazon SageMaker.\n",
      " |      \n",
      " |      This is a synchronous operation. After the model training\n",
      " |      successfully completes, you can call the ``deploy()`` method to host the\n",
      " |      model using the Amazon SageMaker hosting services.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs (str or dict or sagemaker.inputs.TrainingInput): Information\n",
      " |              about the training data. This can be one of three types:\n",
      " |      \n",
      " |              * (str) the S3 location where training data is saved, or a file:// path in\n",
      " |                  local mode.\n",
      " |              * (dict[str, str] or dict[str, sagemaker.inputs.TrainingInput]) If using multiple\n",
      " |                  channels for training data, you can specify a dict mapping channel names to\n",
      " |                  strings or :func:`~sagemaker.inputs.TrainingInput` objects.\n",
      " |              * (sagemaker.inputs.TrainingInput) - channel configuration for S3 data sources\n",
      " |                  that can provide additional information as well as the path to the training\n",
      " |                  dataset.\n",
      " |                  See :func:`sagemaker.inputs.TrainingInput` for full details.\n",
      " |              * (sagemaker.session.FileSystemInput) - channel configuration for\n",
      " |                  a file system data source that can provide additional information as well as\n",
      " |                  the path to the training dataset.\n",
      " |      \n",
      " |          wait (bool): Whether the call should wait until the job completes (default: True).\n",
      " |          logs ([str]): A list of strings specifying which logs to print. Acceptable\n",
      " |              strings are \"All\", \"None\", \"Training\", or \"Rules\". To maintain backwards\n",
      " |              compatibility, boolean values are also accepted and converted to strings.\n",
      " |              Only meaningful when wait is True.\n",
      " |          job_name (str): Training job name. If not specified, the estimator generates\n",
      " |              a default job name based on the training image name and current timestamp.\n",
      " |          experiment_config (dict[str, str]): Experiment management configuration.\n",
      " |              Dictionary contains three optional keys,\n",
      " |              'ExperimentName', 'TrialName', and 'TrialComponentDisplayName'.\n",
      " |  \n",
      " |  get_vpc_config(self, vpc_config_override='VPC_CONFIG_DEFAULT')\n",
      " |      Returns VpcConfig dict either from this Estimator's subnets and security groups.\n",
      " |      \n",
      " |      Or else validate and return an optional override value.\n",
      " |      \n",
      " |      Args:\n",
      " |          vpc_config_override:\n",
      " |  \n",
      " |  latest_job_debugger_artifacts_path(self)\n",
      " |      Gets the path to the DebuggerHookConfig output artifacts.\n",
      " |      \n",
      " |      Returns:\n",
      " |          str: An S3 path to the output artifacts.\n",
      " |  \n",
      " |  latest_job_profiler_artifacts_path(self)\n",
      " |      Gets the path to the profiling output artifacts.\n",
      " |      \n",
      " |      Returns:\n",
      " |          str: An S3 path to the output artifacts.\n",
      " |  \n",
      " |  latest_job_tensorboard_artifacts_path(self)\n",
      " |      Gets the path to the TensorBoardOutputConfig output artifacts.\n",
      " |      \n",
      " |      Returns:\n",
      " |          str: An S3 path to the output artifacts.\n",
      " |  \n",
      " |  logs(self)\n",
      " |      Display the logs for Estimator's training job.\n",
      " |      \n",
      " |      If the output is a tty or a Jupyter cell, it will be color-coded based\n",
      " |      on which instance the log entry is from.\n",
      " |  \n",
      " |  prepare_workflow_for_training(self, job_name=None)\n",
      " |      Calls _prepare_for_training. Used when setting up a workflow.\n",
      " |      \n",
      " |      Args:\n",
      " |          job_name (str): Name of the training job to be created. If not\n",
      " |              specified, one is generated, using the base name given to the\n",
      " |              constructor if applicable.\n",
      " |  \n",
      " |  register(self, content_types, response_types, inference_instances, transform_instances, image_uri=None, model_package_name=None, model_package_group_name=None, model_metrics=None, metadata_properties=None, marketplace_cert=False, approval_status=None, description=None, compile_model_family=None, model_name=None, **kwargs)\n",
      " |      Creates a model package for creating SageMaker models or listing on Marketplace.\n",
      " |      \n",
      " |      Args:\n",
      " |          content_types (list): The supported MIME types for the input data.\n",
      " |          response_types (list): The supported MIME types for the output data.\n",
      " |          inference_instances (list): A list of the instance types that are used to\n",
      " |              generate inferences in real-time.\n",
      " |          transform_instances (list): A list of the instance types on which a transformation\n",
      " |              job can be run or on which an endpoint can be deployed.\n",
      " |          image_uri (str): The container image uri for Model Package, if not specified,\n",
      " |              Estimator's training container image will be used (default: None).\n",
      " |          model_package_name (str): Model Package name, exclusive to `model_package_group_name`,\n",
      " |              using `model_package_name` makes the Model Package un-versioned (default: None).\n",
      " |          model_package_group_name (str): Model Package Group name, exclusive to\n",
      " |              `model_package_name`, using `model_package_group_name` makes the Model Package\n",
      " |              versioned (default: None).\n",
      " |          model_metrics (ModelMetrics): ModelMetrics object (default: None).\n",
      " |          metadata_properties (MetadataProperties): MetadataProperties (default: None).\n",
      " |          marketplace_cert (bool): A boolean value indicating if the Model Package is certified\n",
      " |              for AWS Marketplace (default: False).\n",
      " |          approval_status (str): Model Approval Status, values can be \"Approved\", \"Rejected\",\n",
      " |              or \"PendingManualApproval\" (default: \"PendingManualApproval\").\n",
      " |          description (str): Model Package description (default: None).\n",
      " |          compile_model_family (str): Instance family for compiled model, if specified, a compiled\n",
      " |              model will be used (default: None).\n",
      " |          model_name (str): User defined model name (default: None).\n",
      " |          **kwargs: Passed to invocation of ``create_model()``. Implementations may customize\n",
      " |              ``create_model()`` to accept ``**kwargs`` to customize model creation during\n",
      " |              deploy. For more, see the implementation docs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          str: A string of SageMaker Model Package ARN.\n",
      " |  \n",
      " |  update_profiler(self, rules=None, system_monitor_interval_millis=None, s3_output_path=None, framework_profile_params=None, disable_framework_metrics=False)\n",
      " |      Update training jobs to enable profiling.\n",
      " |      \n",
      " |      This method updates the ``profiler_config`` parameter\n",
      " |      and initiates Debugger built-in rules for profiling.\n",
      " |      \n",
      " |      Args:\n",
      " |          rules (list[:class:`~sagemaker.debugger.ProfilerRule`]): A list of\n",
      " |              :class:`~sagemaker.debugger.ProfilerRule` objects to define\n",
      " |              rules for continuous analysis with SageMaker Debugger. Currently, you can\n",
      " |              only add new profiler rules during the training job. (default: ``None``)\n",
      " |          s3_output_path (str): The location in S3 to store the output. If profiler is enabled\n",
      " |              once, s3_output_path cannot be changed. (default: ``None``)\n",
      " |          system_monitor_interval_millis (int): How often profiling system metrics are\n",
      " |              collected; Unit: Milliseconds (default: ``None``)\n",
      " |          framework_profile_params (:class:`~sagemaker.debugger.FrameworkProfile`):\n",
      " |              A parameter object for framework metrics profiling. Configure it using\n",
      " |              the :class:`~sagemaker.debugger.FrameworkProfile` class.\n",
      " |              To use the default framework profile parameters, pass ``FrameworkProfile()``.\n",
      " |              For more information about the default values,\n",
      " |              see :class:`~sagemaker.debugger.FrameworkProfile`. (default: ``None``)\n",
      " |          disable_framework_metrics (bool): Specify whether to disable all the framework metrics.\n",
      " |              This won't update system metrics and the Debugger built-in rules for monitoring.\n",
      " |              To stop both monitoring and profiling,\n",
      " |              use the :class:`~sagemaker.estimator.Estimator.desable_profiling`\n",
      " |              method. (default: ``False``)\n",
      " |      \n",
      " |      .. attention::\n",
      " |      \n",
      " |          Updating the profiling configuration for TensorFlow dataloader profiling\n",
      " |          is currently not available. If you started a TensorFlow training job only with\n",
      " |          monitoring and want to enable profiling while the training job is running,\n",
      " |          the dataloader profiling cannot be updated.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sagemaker.estimator.EstimatorBase:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  model_data\n",
      " |      str: The model location in S3. Only set if Estimator has been ``fit()``.\n",
      " |  \n",
      " |  training_job_analytics\n",
      " |      Return a ``TrainingJobAnalytics`` object for the current training job.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributions has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "tf_estimator = TensorFlow(entry_point='TransferNetworkTraining.py', role=role, \n",
    "                         train_instance_count=1, train_instance_type='ml.p2.xlarge',\n",
    "                          framework_version='1.12', py_version='py3',\n",
    "                          distributions={\n",
    "                              'mpi': {\n",
    "                                  'enabled': True,\n",
    "                                  'processes_per_host':1,\n",
    "                                  'custom_mpi_options': '--NCCL_DEBUG INFO'\n",
    "                              }\n",
    "                          },\n",
    "                          output_path='s3://otolith-everything/publication_models'\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-04 23:24:55 Starting - Starting the training job...\n",
      "2021-03-04 23:25:19 Starting - Launching requested ML instances......\n",
      "2021-03-04 23:26:19 Starting - Preparing the instances for training............\n",
      "2021-03-04 23:28:21 Downloading - Downloading input data\n",
      "2021-03-04 23:28:21 Training - Downloading the training image...\n",
      "2021-03-04 23:28:41 Training - Training image download completed. Training in progress.\u001b[34m2021-03-04 23:28:41,968 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2021-03-04 23:28:42,305 sagemaker-containers INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2021-03-04 23:28:42,305 sagemaker-containers INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2021-03-04 23:28:42,311 sagemaker-containers INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2021-03-04 23:28:42,311 sagemaker-containers INFO     Env Hosts: ['algo-1'] Hosts: ['algo-1'] process_per_hosts: 1 num_processes: 1\u001b[0m\n",
      "\u001b[34m2021-03-04 23:28:42,317 sagemaker-containers INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2021-03-04 23:28:42,345 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_mpi_num_of_processes_per_host\": 1,\n",
      "        \"sagemaker_mpi_custom_mpi_options\": \"--NCCL_DEBUG INFO\",\n",
      "        \"sagemaker_mpi_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"model_dir\": \"/opt/ml/model\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"TrainJob-WithValidation-v3\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://otolith-everything/TrainJob-WithValidation-v3/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"TransferNetworkTraining\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"TransferNetworkTraining.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"model_dir\":\"/opt/ml/model\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=TransferNetworkTraining.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_mpi_custom_mpi_options\":\"--NCCL_DEBUG INFO\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":1}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=TransferNetworkTraining\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://otolith-everything/TrainJob-WithValidation-v3/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_mpi_custom_mpi_options\":\"--NCCL_DEBUG INFO\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":1},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"model_dir\":\"/opt/ml/model\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"TrainJob-WithValidation-v3\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://otolith-everything/TrainJob-WithValidation-v3/source/sourcedir.tar.gz\",\"module_name\":\"TransferNetworkTraining\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"TransferNetworkTraining.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--model_dir\",\"/opt/ml/model\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1 -np 1 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to socket -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/usr/local/lib/python3.6/dist-packages/gethostname.cpython-36m-x86_64-linux-gnu.so -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_TRAINING -x SM_HP_MODEL_DIR -x PYTHONPATH /usr/bin/python -m mpi4py TransferNetworkTraining.py --model_dir /opt/ml/model\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mWARNING: a request was made to bind a process. While the system\u001b[0m\n",
      "\u001b[34msupports binding the process itself, at least one node does NOT\u001b[0m\n",
      "\u001b[34msupport binding memory to the process location.\n",
      "\n",
      "  Node:  ip-10-0-136-76\n",
      "\u001b[0m\n",
      "\u001b[34mOpen MPI uses the \"hwloc\" library to perform process and memory\u001b[0m\n",
      "\u001b[34mbinding. This error message means that hwloc has indicated that\u001b[0m\n",
      "\u001b[34mprocessor binding support is not available on this machine.\n",
      "\u001b[0m\n",
      "\u001b[34mOn OS X, processor and memory binding is not available at all (i.e.,\u001b[0m\n",
      "\u001b[34mthe OS does not expose this functionality).\n",
      "\u001b[0m\n",
      "\u001b[34mOn Linux, lack of the functionality can mean that you are on a\u001b[0m\n",
      "\u001b[34mplatform where processor and memory affinity is not supported in Linux\u001b[0m\n",
      "\u001b[34mitself, or that hwloc was built without NUMA and/or processor affinity\u001b[0m\n",
      "\u001b[34msupport. When building hwloc (which, depending on your Open MPI\u001b[0m\n",
      "\u001b[34minstallation, may be embedded in Open MPI itself), it is important to\u001b[0m\n",
      "\u001b[34mhave the libnuma header and library files available. Different linux\u001b[0m\n",
      "\u001b[34mdistributions package these files under different names; look for\u001b[0m\n",
      "\u001b[34mpackages with the word \"numa\" in them. You may also need a developer\u001b[0m\n",
      "\u001b[34mversion of the package (e.g., with \"dev\" or \"devel\" in the name) to\n",
      " Data for JOB [3967,1] offset 0 Total slots allocated 1\n",
      "\n",
      " ========================   JOB MAP   ========================\n",
      "\n",
      " Data for node: ip-10-0-136-76#011Num slots: 1#011Max slots: 0#011Num procs: 1\n",
      " #011Process OMPI jobid: [3967,1] App: 0 Process rank: 0 Bound: UNBOUND\n",
      "\n",
      " =============================================================\u001b[0m\n",
      "\u001b[34mobtain the relevant header files.\n",
      "\u001b[0m\n",
      "\u001b[34mIf you are getting this message on a non-OS X, non-Linux platform,\u001b[0m\n",
      "\u001b[34mthen hwloc does not support processor / memory affinity on this\u001b[0m\n",
      "\u001b[34mplatform. If the OS/platform does actually support processor / memory\u001b[0m\n",
      "\u001b[34maffinity, then you should contact the hwloc maintainers:\u001b[0m\n",
      "\u001b[34mhttps://github.com/open-mpi/hwloc.\n",
      "\u001b[0m\n",
      "\u001b[34mThis is a warning only; your job will continue, though performance may\u001b[0m\n",
      "\u001b[34mbe degraded.\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Starting the training.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:cv order: [ 14  98  75  16 131  56 141  44  29 120  94   5 102  51  78  42  92  66\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  31  35  90  84  77  40 125  99  33  19  73 146  91 135  69 128 114  48\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  53  28  54 108 112  17 119 103  58 118  18   4  45  59  39  36 117 139\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 107 132 126  85 122  95  11 113 123  12   2 104   6 127 110  65  55 144\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: 138  46  62  74 116  93 100  89  10  34  32 124  38  83 111 149  27  23\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  67   9 130  97 105 145  87 148 109  64  15  82  41  80  52  26  76  43\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  24 136 121 143  49  21  70   3 142  30 147 106  47 115  13  88   8  81\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  60   0   1  57  22  61  63   7  86  96  68  50 101  20  25 134  71 129\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:  79 133 137  72 140  37]\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:---------------------Training file name:  /opt/ml/input/data/training/publication_classification_training_array.p\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:tr shape: (82860, 151, 1)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:tr shape: (82860, 151, 1)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:tr shape: (82860, 151, 1)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:tr shape: (82860, 151, 1)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:tr shape: (82860, 151, 1)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:tr shape: (82860, 151, 1)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:tr shape: (82860, 151, 1)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:tr shape: (82860, 151, 1)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:tr shape: (82860, 151, 1)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:tr shape: (82860, 151, 1)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Training complete.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:---------------------Training file name:  /opt/ml/input/data/training/publication_binary_training_array.p\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:tr shape: (136860, 151, 1)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:tr shape: (136860, 151, 1)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:tr shape: (136860, 151, 1)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:tr shape: (136860, 151, 1)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:tr shape: (136860, 151, 1)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:tr shape: (136860, 151, 1)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:tr shape: (136860, 151, 1)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:tr shape: (136860, 151, 1)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:tr shape: (136860, 151, 1)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:tr shape: (136860, 151, 1)\u001b[0m\n",
      "\n",
      "2021-03-05 02:22:10 Uploading - Uploading generated training model\u001b[34m[1,0]<stdout>:Training complete.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Training complete.\u001b[0m\n",
      "\u001b[34m2021-03-05 02:22:04,427 sagemaker_tensorflow_container.training WARNING  Your model will NOT be servable with SageMaker TensorFlow Serving container.The model artifact was not saved in the TensorFlow SavedModel directory structure:\u001b[0m\n",
      "\u001b[34mhttps://www.tensorflow.org/guide/saved_model#structure_of_a_savedmodel_directory\u001b[0m\n",
      "\u001b[34m2021-03-05 02:22:04,428 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-03-05 02:24:12 Completed - Training job completed\n",
      "Training seconds: 10574\n",
      "Billable seconds: 10574\n"
     ]
    }
   ],
   "source": [
    "tf_estimator.fit('s3://otolith-everything/publication_training', logs='Training', job_name='TrainJob-WithValidation-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " [ 14  98  75  16 131  56 141  44  29 120  94   5 102  51  78  42  92  66\n",
    "[1,0]<stdout>:  31  35  90  84  77  40 125  99  33  19  73 146  91 135  69 128 114  48\n",
    "[1,0]<stdout>:  53  28  54 108 112  17 119 103  58 118  18   4  45  59  39  36 117 139\n",
    "[1,0]<stdout>: 107 132 126  85 122  95  11 113 123  12   2 104   6 127 110  65  55 144\n",
    "[1,0]<stdout>: 138  46  62  74 116  93 100  89  10  34  32 124  38  83 111 149  27  23\n",
    "[1,0]<stdout>:  67   9 130  97 105 145  87 148 109  64  15  82  41  80  52  26  76  43\n",
    "[1,0]<stdout>:  24 136 121 143  49  21  70   3 142  30 147 106  47 115  13  88   8  81\n",
    "[1,0]<stdout>:  60   0   1  57  22  61  63   7  86  96  68  50 101  20  25 134  71 129\n",
    "[1,0]<stdout>:  79 133 137  72 140  37]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
